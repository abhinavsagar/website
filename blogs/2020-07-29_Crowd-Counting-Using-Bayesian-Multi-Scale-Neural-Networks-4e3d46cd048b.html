<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Crowd Counting Using Bayesian Multi Scale Neural Networks</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Crowd Counting Using Bayesian Multi Scale Neural Networks</h1>
</header>
<section data-field="subtitle" class="p-summary">
Combining Convolutional Neural Networks and Uncertainty Quantification
</section>
<section data-field="body" class="e-content">
<section name="f575" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="d3ec" id="d3ec" class="graf graf--h3 graf--leading graf--title">Crowd Counting Using Bayesian Multi Scale Neural Networks</h3><h4 name="36fc" id="36fc" class="graf graf--h4 graf-after--h3 graf--subtitle">Combining Convolutional Neural Networks and Uncertainty Quantification</h4></div><div class="section-inner sectionLayout--fullWidth"><figure name="f36b" id="f36b" class="graf graf--figure graf--layoutFillWidth graf-after--h4"><img class="graf-image" data-image-id="1*BbQLz2nTVbEmhIbPzRrAGw.jpeg" data-width="6016" data-height="4016" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2560/1*BbQLz2nTVbEmhIbPzRrAGw.jpeg"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@chuttersnap?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/@chuttersnap?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">chuttersnap</a> on <a href="https://unsplash.com/s/photos/crowd?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/s/photos/crowd?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="591e" id="591e" class="graf graf--p graf-after--figure">Convolutional Neural Networks based on estimating the density map over the image has been highly successful for crowd counting. However dense crowd counting remains an open problem because of severe occlusion and perspective view in which people can be present at various shape and sizes. This blog presents our research work done on Crowd Counting by combining Convolutional Neural Networks and uncertainty quantification.</p><h3 name="5e9f" id="5e9f" class="graf graf--h3 graf-after--p">Important Points</h3><ol class="postList"><li name="82ce" id="82ce" class="graf graf--li graf-after--h3">We propose a new network which uses a ResNet based feature extractor, downsampling block using dilated convolutions and upsampling block using transposed convolutions.</li><li name="78d2" id="78d2" class="graf graf--li graf-after--li">We present a novel aggregation module which makes our network robust to the perspective view problem.</li><li name="8e08" id="8e08" class="graf graf--li graf-after--li">We present the optimization details, loss functions and the algorithm used in our work.</li><li name="e873" id="e873" class="graf graf--li graf-after--li">We used ShanghaiTech, UCF-CC-50 and UCF-QNRF datasets for training and testing.</li><li name="fca9" id="fca9" class="graf graf--li graf-after--li">Using MSE and MAE as evaluation metrics, our network outperforms previous state-of-the-art approaches while giving uncertainty estimates in a principled bayesian manner.</li></ol><h3 name="460e" id="460e" class="graf graf--h3 graf-after--li">Introduction</h3><p name="ac2a" id="ac2a" class="graf graf--p graf-after--h3">Crowd Counting has a range of applications like counting the number of participants in political rallies, social and sports events, etc.</p><p name="3fc4" id="3fc4" class="graf graf--p graf-after--p">Crowd Counting is a difficult problem especially in dense crowds due to two main reasons:</p><ol class="postList"><li name="ba7c" id="ba7c" class="graf graf--li graf-after--p">There is often clutter, overlap and occlusions present.</li><li name="a7a8" id="a7a8" class="graf graf--li graf-after--li">In perspective view it is difficult to take into account the shape and size of object present with respect to the background.</li></ol><p name="b811" id="b811" class="graf graf--p graf-after--li">A lot of algorithms have been proposed in the literature for tackling this problem. Most of them use some form of convolutional neural network along with a density map estimation which predicts a density map over the input image and then summing to get the count of objects.</p><h3 name="d910" id="d910" class="graf graf--h3 graf-after--p">Datasets</h3><p name="5321" id="5321" class="graf graf--p graf-after--h3">The following datasets were used in this work for training and testing the network:</p><ol class="postList"><li name="bd6f" id="bd6f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">ShanghaiTech</strong> is made up of two datasets labelled as part A and part B. In part A, there are 300 images for training and 182 images for testing while Part B has 400 training images and 316 testing images.</li><li name="d261" id="d261" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">UCF-CC-50</strong> contains 50 gray images with different resolutions. The average count for each image is 1,280, and the minimum and maximum counts are 94 and 4,532, respectively.</li><li name="4c76" id="4c76" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">UCF-QNRF</strong> is the third dataset used in this work which has 1535 images with 1.25 million point annotations. The training set has 1,201 images and 334 images are used for testing.</li></ol><h3 name="3752" id="3752" class="graf graf--h3 graf-after--li">Network Architecture</h3><p name="b655" id="b655" class="graf graf--p graf-after--h3">The network architecture used in this work is described in the following points:</p><ol class="postList"><li name="70be" id="70be" class="graf graf--li graf-after--p">A ResNet based feature extractor is used with dilated convolutions which is defined as a downsampling block. This helps in extracting the details of objects at various scales hence solving the perspective view problem faced by earlier approaches.</li><li name="c5da" id="c5da" class="graf graf--li graf-after--li">The upsampling block uses transposed convolutions with skip connections in between the two creating an additional pathway thus avoiding overfitting.</li><li name="79a8" id="79a8" class="graf graf--li graf-after--li">The last part has three heads: output of density map which when integrated gives the absolute count, epistemic uncertainty and aleatoric uncertainty heads.</li></ol><p name="67dd" id="67dd" class="graf graf--p graf-after--li">The network architecture along with layerwise details used in this work is shown in Figure 1:</p><figure name="25dd" id="25dd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QyguiCvMWqmIKnwpDym59Q.png" data-width="589" data-height="387" src="https://cdn-images-1.medium.com/max/800/1*QyguiCvMWqmIKnwpDym59Q.png"><figcaption class="imageCaption">Figure 1: Our Neural network architecture</figcaption></figure><p name="ed3f" id="ed3f" class="graf graf--p graf-after--figure">Where 1×1, 3×3 denotes Filters, 64, 128, 256 denotes Receptive Field, conv denotes Dilated Convolutional layer and conv-2 denotes Transposed convolutional layer.</p><h3 name="ff35" id="ff35" class="graf graf--h3 graf-after--p">Optimization</h3><p name="c3d2" id="c3d2" class="graf graf--p graf-after--h3">To solve the vanishing gradient problem, instance normalization was used after both dilated convolutional and transposed convolutional layers as defined in Equation 1:</p><figure name="5725" id="5725" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nCJZUbU2A5A3OpKkx1EGZA.png" data-width="886" data-height="114" src="https://cdn-images-1.medium.com/max/800/1*nCJZUbU2A5A3OpKkx1EGZA.png"></figure><p name="ffe8" id="ffe8" class="graf graf--p graf-after--figure">where w and b are weight and bias term of the convolution layer, γ and β are weight and bias term of the Instance Normalization layer, µ and σ are mean and variance of the input.</p><p name="03ae" id="03ae" class="graf graf--p graf-after--p">We propose a new technique to aggregate the filters with sizes 1×1, 3×3, 5×5. ReLU is applied after every convolutional and transposed convolutional layer. Our novel aggregation module used is shown in Figure 2:</p><figure name="50cd" id="50cd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qaUV8sIxs7w0jzAGHorjbg.png" data-width="499" data-height="302" src="https://cdn-images-1.medium.com/max/800/1*qaUV8sIxs7w0jzAGHorjbg.png"><figcaption class="imageCaption">Figure 2: The architecture of our aggregation module</figcaption></figure><p name="72b2" id="72b2" class="graf graf--p graf-after--figure">The filter branches make our network robust and can be extended by using more filters to tackle crowd counting in dense scenes. Our aggregation modules stacked on top of each other behave as ensembles thus minimizing overfitting which is a challenge while training deep networks.</p><h3 name="1c87" id="1c87" class="graf graf--h3 graf-after--p">Loss Functions</h3><p name="b708" id="b708" class="graf graf--p graf-after--h3">Most existing work uses pixelwise Euclidean loss for training the network. This gives a measure of estimation error at pixel level which is defined in Equation 2:</p><figure name="cdc0" id="cdc0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lhqwl356FgknfTwUAKWeyg.png" data-width="970" data-height="90" src="https://cdn-images-1.medium.com/max/800/1*lhqwl356FgknfTwUAKWeyg.png"></figure><p name="ed75" id="ed75" class="graf graf--p graf-after--figure">where θ denotes a set of the network parameters, N is the number of pixels in density maps, X is the input image and Y is the corresponding ground truth density map, F(X, θ) denotes the estimated density map.</p><p name="764d" id="764d" class="graf graf--p graf-after--p">We also incorporate SSIM index in our loss to measure the deviation of the prediction from the ground truth. It computes similarity between two images from three local statistics, i.e. mean, variance and covariance. The range of SSIM values is from -1 to 1 and it is equal to 1 when the two images are identical. SSIM index is defined in Equation 3:</p><figure name="a6ca" id="a6ca" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*I7nk7NAFXYWDpuJqaMLYZg.png" data-width="883" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*I7nk7NAFXYWDpuJqaMLYZg.png"></figure><p name="87ca" id="87ca" class="graf graf--p graf-after--figure">where C1 and C2 are small constants to avoid division by zero. The loss function can be written by averaging over the integral as shown in Equation 4:</p><figure name="44b6" id="44b6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UnOnmq_uxGq42CNwN0B2Ow.png" data-width="879" data-height="120" src="https://cdn-images-1.medium.com/max/800/1*UnOnmq_uxGq42CNwN0B2Ow.png"></figure><p name="ddcf" id="ddcf" class="graf graf--p graf-after--figure">where N is the number of pixels in density maps. LS gives a measure of the difference between the network predictions and ground truth. The final loss function by adding the two terms can be written as shown in Equation 5:</p><figure name="dd85" id="dd85" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*obzApc37Xw8MlfkYjSJ8vA.png" data-width="859" data-height="92" src="https://cdn-images-1.medium.com/max/800/1*obzApc37Xw8MlfkYjSJ8vA.png"></figure><p name="2756" id="2756" class="graf graf--p graf-after--figure">where αC and αS are constants. In our experiments, we set both αC and αS as 0.5 to give equal weightage to both the terms.</p><h3 name="5950" id="5950" class="graf graf--h3 graf-after--p">Evaluation Metrics</h3><p name="8bcc" id="8bcc" class="graf graf--p graf-after--h3">For crowd counting, Mean Absolute Error (MAE) and Mean Squared Error (MSE) are commonly used for quantitative comparison. These metrics are defined in Equation 6 and Equation 7 respectively:</p><figure name="a8e9" id="a8e9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*7uBPI0A0T6Kq4uyJ2MCIgw.png" data-width="955" data-height="310" src="https://cdn-images-1.medium.com/max/800/1*7uBPI0A0T6Kq4uyJ2MCIgw.png"></figure><p name="18a9" id="18a9" class="graf graf--p graf-after--figure">where N is the number of test samples, Ci and CGTi are the estimated and ground truth count corresponding to the i th sample which is given by the integration of the density map.</p><p name="05d1" id="05d1" class="graf graf--p graf-after--p">MAE shows the accuracy of predicted result while MSE measures the robustness of prediction.</p><h3 name="e1e3" id="e1e3" class="graf graf--h3 graf-after--p">Uncertainty Estimation</h3><p name="4016" id="4016" class="graf graf--p graf-after--h3">There are two main sources of uncertainty in model predictions: epistemic uncertainty is uncertainty due to our lack of knowledge and aleatoric uncertainty is due to stochasticity present in the data. Epistemic uncertainty is often called model uncertainty and it can be explained away given enough data. Using bayesian neural networks in which the weights are parameterized by distributions instead of point estimates, epistemic uncertainty can be computed. However crowd counting requires understanding the inherent nuances of the data like occlusions, scale ambiguity etc, hence aleatoric uncertainty is also important.</p><h3 name="36fb" id="36fb" class="graf graf--h3 graf-after--p">Algorithm</h3><p name="48b4" id="48b4" class="graf graf--p graf-after--h3">The algorithm used in this work is shown below:</p><figure name="e510" id="e510" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*bW65PSWR3vCuo0a85z6Dyw.png" data-width="1115" data-height="414" src="https://cdn-images-1.medium.com/max/800/1*bW65PSWR3vCuo0a85z6Dyw.png"></figure><h3 name="e971" id="e971" class="graf graf--h3 graf-after--figure">Experimental Results</h3><p name="58ec" id="58ec" class="graf graf--p graf-after--h3">As shown in Table 2, our method obtains the lowest Mean Square Error (MSE) and Mean Absolute Error (MAE) on both subset of ShanghaiTech dataset.</p><figure name="4810" id="4810" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5AdY5nogkudC1hosoieBNA.png" data-width="1018" data-height="456" src="https://cdn-images-1.medium.com/max/800/1*5AdY5nogkudC1hosoieBNA.png"></figure><p name="a0a3" id="a0a3" class="graf graf--p graf-after--figure">As shown in Table 3, our method obtains the lowest MSE and MAE on UCF CC 50 dataset.</p><figure name="2301" id="2301" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yOZJTa--tvHLmtrlPE_13w.png" data-width="992" data-height="488" src="https://cdn-images-1.medium.com/max/800/1*yOZJTa--tvHLmtrlPE_13w.png"></figure><p name="e62c" id="e62c" class="graf graf--p graf-after--figure">As shown in Table 4, our method obtains the lowest MSE and MAE on UCF-QNRF dataset.</p><figure name="e8f0" id="e8f0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IoGT7tHRcvj02-qWNsPBLw.png" data-width="1017" data-height="376" src="https://cdn-images-1.medium.com/max/800/1*IoGT7tHRcvj02-qWNsPBLw.png"></figure><p name="1ce2" id="1ce2" class="graf graf--p graf-after--figure">The number of parameters of our proposed network is the least compared to previous works as shown in Table 5:</p><figure name="d31f" id="d31f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*A5YKyEkaLJSrob_bhr4hAw.png" data-width="1260" data-height="177" src="https://cdn-images-1.medium.com/max/800/1*A5YKyEkaLJSrob_bhr4hAw.png"></figure><p name="a7f4" id="a7f4" class="graf graf--p graf-after--figure">Figure 3 and Figure 4 respectively illustrate the qualitative results for sample images from the ShanghaiTech and UCF-QNFRF datasets respectively.</p><figure name="c3ca" id="c3ca" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*x2G2sAnXBAsU8DifNMYQIQ.png" data-width="942" data-height="275" src="https://cdn-images-1.medium.com/max/800/1*x2G2sAnXBAsU8DifNMYQIQ.png"><figcaption class="imageCaption">Figure 3: Sample results of the proposed method on ShanghaiTech dataset (a) Input. (b) Ground truth © Estimated density map (d) epistemic uncertainty and (e) aleatoric uncertainty quantification.</figcaption></figure><figure name="0f80" id="0f80" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*GiC6BUlcy0YoQh_JFM01jw.png" data-width="1016" data-height="311" src="https://cdn-images-1.medium.com/max/800/1*GiC6BUlcy0YoQh_JFM01jw.png"><figcaption class="imageCaption">Figure 4: Sample results of the proposed method on UCF-QNRF dataset (a) Input. (b) Ground truth © Estimated density map (d) epistemic uncertainty and (e) aleatoric uncertainty quantification.</figcaption></figure><p name="f4ca" id="f4ca" class="graf graf--p graf-after--figure">More red color represents higher uncertainty.</p><p name="55f5" id="55f5" class="graf graf--p graf-after--p">The two conclusions to be drawn from the above two figures are:</p><ol class="postList"><li name="1080" id="1080" class="graf graf--li graf-after--p">Both epistemic uncertainty and aleatoric uncertainty are corelated especially where the crowd density is high.</li><li name="a946" id="a946" class="graf graf--li graf-after--li">The model is less certain in dense crowds hence uncertainty is high in those locations.</li></ol><h3 name="aac5" id="aac5" class="graf graf--h3 graf-after--li">Conclusions</h3><p name="12ae" id="12ae" class="graf graf--p graf-after--h3">In this blog, we presented a novel neural network for crowd counting which is based on a ResNet based feature extractor and a new feature aggregation module. The downsampling blocks use dilated convolutional layers while upsampling blocks use transposed convolutional layers. Skip connections in between the blocks create an additional pathway thus preventing overfitting. We presented the optimization details, loss functions and algorithms used in this work. Our method not only outperforms previous state of the art methods but also gives a measure of uncertainty thus solving the famous black box problem of neural networks.</p><h3 name="6e4a" id="6e4a" class="graf graf--h3 graf-after--p">References</h3><p name="4f47" id="4f47" class="graf graf--p graf-after--h3">H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed, N. Rajpoot, and M. Shah. Composition loss for counting, density map estimation and localization in dense crowds. In Proceedings of the European Conference on Computer Vision (ECCV), pages 532–546, 2018.</p><p name="42ef" id="42ef" class="graf graf--p graf-after--p">X. Jiang, Z. Xiao, B. Zhang, X. Zhen, X. Cao, D. Doermann, and L. Shao. Crowd counting and density estimation by trellis encoder-decoder networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6133–6142, 2019.</p><p name="7942" id="7942" class="graf graf--p graf-after--p">D. Kang and A. Chan. Crowd counting by adaptively fusing predictions from an image pyramid. arXiv preprint arXiv:1805.06115, 2018.</p><p name="753e" id="753e" class="graf graf--p graf-after--p">L. Zhang, M. Shi, and Q. Chen. Crowd counting via scale-adaptive convolutional neural network. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1113–1121. IEEE, 2018.</p><p name="093e" id="093e" class="graf graf--p graf-after--p">D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.</p><h3 name="cff0" id="cff0" class="graf graf--h3 graf-after--p">Before You Go</h3><p name="a4ff" id="a4ff" class="graf graf--p graf-after--h3">Research Paper:<strong class="markup--strong markup--p-strong"> </strong><a href="https://arxiv.org/pdf/2007.14245.pdf" data-href="https://arxiv.org/pdf/2007.14245.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/2007.14245.pdf</a></p><p name="9299" id="9299" class="graf graf--p graf-after--p graf--trailing">Code:<strong class="markup--strong markup--p-strong"> </strong><a href="https://github.com/abhinavsagar/bmsnn" data-href="https://github.com/abhinavsagar/bmsnn" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/abhinavsagar/bmsnn</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@abhinav.sagar" class="p-author h-card">Abhinav Sagar</a> on <a href="https://medium.com/p/4e3d46cd048b"><time class="dt-published" datetime="2020-07-29T17:33:30.561Z">July 29, 2020</time></a>.</p><p><a href="https://medium.com/@abhinav.sagar/crowd-counting-using-bayesian-multi-scale-neural-networks-4e3d46cd048b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 28, 2021.</p></footer></article></body></html>