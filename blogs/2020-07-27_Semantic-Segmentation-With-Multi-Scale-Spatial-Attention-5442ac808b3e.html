<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Semantic Segmentation With Multi Scale Spatial Attention</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Semantic Segmentation With Multi Scale Spatial Attention</h1>
</header>
<section data-field="subtitle" class="p-summary">
New state of the art neural network for autonomous driving
</section>
<section data-field="body" class="e-content">
<section name="4ac4" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3b01" id="3b01" class="graf graf--h3 graf--leading graf--title">Semantic Segmentation With Multi Scale Spatial Attention</h3><h4 name="1ed5" id="1ed5" class="graf graf--h4 graf-after--h3 graf--subtitle">New state of the art neural network for autonomous driving</h4></div><div class="section-inner sectionLayout--fullWidth"><figure name="deb0" id="deb0" class="graf graf--figure graf--layoutFillWidth graf-after--h4"><img class="graf-image" data-image-id="1*pb5jjNowzXvUQNEubPGvVQ.jpeg" data-width="3000" data-height="2000" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2560/1*pb5jjNowzXvUQNEubPGvVQ.jpeg"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@ort?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/@ort?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Bram Van Oost</a> on <a href="https://unsplash.com/s/photos/tesla?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/s/photos/tesla?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="9c98" id="9c98" class="graf graf--p graf-after--figure">This blog presents a novel neural network using multi scale feature fusion at different scales for accurate and efficient semantic segmentation.</p><h3 name="68a2" id="68a2" class="graf graf--h3 graf-after--p">Important Points</h3><ol class="postList"><li name="f3f2" id="f3f2" class="graf graf--li graf-after--h3">We have used dilated convolutional layers in downsampling part, transposed convolutional layers in the upsampling part and concat layers to merge them.</li><li name="1dd8" id="1dd8" class="graf graf--li graf-after--li">Skip connections in between alternate blocks are used which helps in reducing overfitting considerably.</li><li name="c74a" id="c74a" class="graf graf--li graf-after--li">We present an in depth theoretical analysis of our network with training and optimization details.</li><li name="0f19" id="0f19" class="graf graf--li graf-after--li">We evaluated our network on the Camvid dataset using mean accuracy per class and Intersection Over Union (IOU) as the evaluation metrics.</li><li name="be47" id="be47" class="graf graf--li graf-after--li">Our model outperforms previous state of the art networks on semantic segmentation achieving mean IOU value of 74.12 while running at &gt;100 FPS.</li></ol><h3 name="54bd" id="54bd" class="graf graf--h3 graf-after--li">Semantic Segmentation</h3><p name="b81f" id="b81f" class="graf graf--p graf-after--h3">Semantic segmentation requires predicting a class for each and every pixel of the input image, instead of only discrete classes for the whole input images. In order to predict what is present in the image for each and every pixel, segmentation needs to find not only what is in the input image, but also where it is. The applications of semantic segmentation autonomous driving, video surveillance, medical imaging etc. This is a challenging problem as there is a tradeoff between accuracy and speed. Since the model eventually needs to be deployed in real world setting, both accuracy and speed should be high.</p><h3 name="ac3c" id="ac3c" class="graf graf--h3 graf-after--p">Dataset</h3><p name="b390" id="b390" class="graf graf--p graf-after--h3">For training and evaluation, Cambridge-driving Labeled Video Database (CamVid) dataset is used. The database provides ground truth labels that associate each pixel with one of 32 classes. The images are of size 360×480. A sample ground truth image from dataset is shown in Fig 1:</p><figure name="1b38" id="1b38" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2sb-4dNeHle0etpuGbGBRQ.png" data-width="351" data-height="271" src="https://cdn-images-1.medium.com/max/800/1*2sb-4dNeHle0etpuGbGBRQ.png"><figcaption class="imageCaption">Figure 1: Ground truth image from dataset</figcaption></figure><p name="a8db" id="a8db" class="graf graf--p graf-after--figure">The original images are taken as ground truth. For any algorithm, the metrics are always evaluated in comparison to the ground truth data. The ground truth information is provided in the dataset for the training and test set. For semantic segmentation problems, the ground truth includes the image, the classes of the objects in it and a segmentation mask for each and every object present in a particular image. These images are shown in binary format for each of the twelve classes separately in Fig 2:</p><figure name="e7d0" id="e7d0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DbrU80ce6q3sgxndg_xiYQ.png" data-width="511" data-height="346" src="https://cdn-images-1.medium.com/max/800/1*DbrU80ce6q3sgxndg_xiYQ.png"><figcaption class="imageCaption">Figure 2: Images converted to binary class mask</figcaption></figure><p name="5414" id="5414" class="graf graf--p graf-after--figure">The classes are Sky, Building, Pole, Road, Pavement, Tree, SignSymbol, Fence, Car, Pedestrian and Bicyclist.</p><h3 name="9dd6" id="9dd6" class="graf graf--h3 graf-after--p">Network Architecture</h3><p name="3a00" id="3a00" class="graf graf--p graf-after--h3">We split the dataset into 2 parts with 85 percent images in the training set and 15 percent images in the test set. The loss function used is categorical cross entropy. We used dilated convolutions in downsampling part to reduce the feature maps and atrous convolutions in upsampling part to recover back the features. Concat operation is used to merge the features at different scales thus encoding more contextual information. We use our very own designed attention module for enlarging the receptive field and encode more contextual information. The attention module used in this work is shown in Figure 3:</p><figure name="df65" id="df65" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iSkNubRX6yC6unQtLpjsHg.png" data-width="505" data-height="297" src="https://cdn-images-1.medium.com/max/800/1*iSkNubRX6yC6unQtLpjsHg.png"><figcaption class="imageCaption">Figure 3: Illustration of our attention module. Here x denotes matrix multiplication and + denotes element wise sum. C, W and H respectively denotes channel, width and height of a layer respectively</figcaption></figure><p name="23bf" id="23bf" class="graf graf--p graf-after--figure">For the dilated convolutional layer we didn’t use any padding, used 3×3 filter and use relu as the activation function. For the max pooling layer, we used 2×2 filters and strides of 2×2. ResNet is used as the backbone and the feature extractor. In the upsampling path we used atrous convolutions layers with 4×4 kernel size and strides of 4×4. Softmax is used as the activation function in the last layer to output discrete probabilities of whether an object is present in a particular pixel location or not. We used Adam as the optimizer for training our network. The network architecture used in this work is shown in Figure 4:</p><figure name="d865" id="d865" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MT7587--gnmRXXIzS57bfA.png" data-width="897" data-height="389" src="https://cdn-images-1.medium.com/max/800/1*MT7587--gnmRXXIzS57bfA.png"><figcaption class="imageCaption">Figure 4: Illustration of our neural network architecture. Here dil-conv represents dilated convolutions and atr-conv represents atrous convolutions. attention1 and attention2 are the two channel wise attention modules used in this work.</figcaption></figure><h3 name="e9ec" id="e9ec" class="graf graf--h3 graf-after--figure">Optimization</h3><p name="5a6a" id="5a6a" class="graf graf--p graf-after--h3">Suppose given a local feature C, we feed it into a convolution layers to generate two new feature maps B and C respectively. We perform a matrix multiplication between the transpose of A and B, and apply a softmax layer to calculate the spatial attention map as defined in the equation below:</p><figure name="e02f" id="e02f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*E1ggi2H3-dpg1Rk-Wcpr3w.png" data-width="340" data-height="101" src="https://cdn-images-1.medium.com/max/800/1*E1ggi2H3-dpg1Rk-Wcpr3w.png"></figure><p name="63ad" id="63ad" class="graf graf--p graf-after--figure">We perform a matrix multiplication between the transpose of X and A and reshape their results. Then we multiply the result by a scale parameter β and perform an element-wise sum operation with A to obtain the final output as shown in the equation below:</p><figure name="4b82" id="4b82" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*W_zA80jB5iIauPUgjX35RA.png" data-width="376" data-height="108" src="https://cdn-images-1.medium.com/max/800/1*W_zA80jB5iIauPUgjX35RA.png"></figure><p name="5cf1" id="5cf1" class="graf graf--p graf-after--figure">The above equation shows that the resultant feature of each channel is a weighted sum of the features of all channels and models the semantic dependencies between feature maps at various scales. For a single backbone, a stage process, the stage in the previous backbone network and sub-stage aggregation method can be formulated as shown in the equation below:</p><figure name="0282" id="0282" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vvmyzqM4nTo0AO4TReJyaw.png" data-width="518" data-height="119" src="https://cdn-images-1.medium.com/max/800/1*vvmyzqM4nTo0AO4TReJyaw.png"></figure><p name="1bc8" id="1bc8" class="graf graf--p graf-after--figure">Here i refers to the index of the stage.</p><h3 name="aad7" id="aad7" class="graf graf--h3 graf-after--p">Experiments</h3><p name="99e8" id="99e8" class="graf graf--p graf-after--h3">The effect of the number of pooling layers on Intersection Over Union(IOU) is shown in Table 2.</p><figure name="787e" id="787e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Drots3g8T47x6uk5gqI9QA.png" data-width="1087" data-height="354" src="https://cdn-images-1.medium.com/max/800/1*Drots3g8T47x6uk5gqI9QA.png"></figure><p name="5d6a" id="5d6a" class="graf graf--p graf-after--figure">The effect of varying the number of branches and fusion methods used in model architecture on IOU is shown in Table 3.</p><figure name="bccd" id="bccd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*SRLq15k9qWfXxtSq68wJrw.png" data-width="1048" data-height="249" src="https://cdn-images-1.medium.com/max/800/1*SRLq15k9qWfXxtSq68wJrw.png"></figure><p name="ccd3" id="ccd3" class="graf graf--p graf-after--figure">The model is trained for 40 epochs, training mean pixel accuracy of 93 percent and validation mean pixel accuracy of 88 percent is achieved. The loss and pixel wise accuracy (both training and test) are plotted as a function of epochs in Fig 4:</p><figure name="7b84" id="7b84" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*kwng1h6p9bxdXKfYFpyPhQ.png" data-width="746" data-height="291" src="https://cdn-images-1.medium.com/max/800/1*kwng1h6p9bxdXKfYFpyPhQ.png"><figcaption class="imageCaption">Figure 4: a) Loss vs epochs b) Accuracy vs epochs</figcaption></figure><h3 name="11c7" id="11c7" class="graf graf--h3 graf-after--figure">Evaluation Metrics</h3><p name="8c4a" id="8c4a" class="graf graf--p graf-after--h3">For evaluation, the following two metrics were used:</p><p name="1c78" id="1c78" class="graf graf--p graf-after--p">1. Mean Accuracy per-class — This metric outputs the class wise prediction accuracy per pixel.</p><p name="ee43" id="ee43" class="graf graf--p graf-after--p">2. Mean IOU — It is a segmentation performance parameter that measures the overlap between two objects by calculating the ratio of intersection and union with ground truth masks.</p><p name="c37f" id="c37f" class="graf graf--p graf-after--p">The class wise IOU values were calculated using the equation below.</p><figure name="98b4" id="98b4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tapXL9_fgDLdp4MnqnByTg.png" data-width="363" data-height="97" src="https://cdn-images-1.medium.com/max/800/1*tapXL9_fgDLdp4MnqnByTg.png"></figure><p name="3668" id="3668" class="graf graf--p graf-after--figure">Where TP denotes true positive, FP denotes false positive, FN denotes false negative and IOU denotes Intersection over union value.</p><h3 name="7497" id="7497" class="graf graf--h3 graf-after--p">Results</h3><p name="4a20" id="4a20" class="graf graf--p graf-after--h3">The effect of using multiple blocks, FLOPS and parameters on IOU is shown in Table 5. Here FLOPS and parameters are a measure of computation required by our model architecture.</p><figure name="4604" id="4604" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YLCxHO1GMEJIGUGNkOQfPw.png" data-width="1098" data-height="415" src="https://cdn-images-1.medium.com/max/800/1*YLCxHO1GMEJIGUGNkOQfPw.png"></figure><p name="55a9" id="55a9" class="graf graf--p graf-after--figure">A comparative analysis on FPS and IOU achieved by previous state of the art model architectures vs ours is shown in Table 6.</p><figure name="fe54" id="fe54" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RhdpLFbZBpHvnBAh2hU_2w.png" data-width="1086" data-height="487" src="https://cdn-images-1.medium.com/max/800/1*RhdpLFbZBpHvnBAh2hU_2w.png"></figure><p name="4215" id="4215" class="graf graf--p graf-after--figure">The results comparing the predicted segmentation vs ground truth image from dataset is shown in Fig 5.</p><figure name="b7a5" id="b7a5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XFLfc1Mz0nA3bKn2MWzIdg.png" data-width="738" data-height="562" src="https://cdn-images-1.medium.com/max/800/1*XFLfc1Mz0nA3bKn2MWzIdg.png"><figcaption class="imageCaption">Figure 5: Results for predicted image —first column original image from dataset, second column predicted image from our network and third column ground truth image from dataset</figcaption></figure><h3 name="ef1f" id="ef1f" class="graf graf--h3 graf-after--figure">Conclusions</h3><p name="928b" id="928b" class="graf graf--p graf-after--h3">In this paper, we proposed a semantic segmentation network using multi scale attention feature maps and evaluated its performance on Camvid dataset. We used a downsampling and upsampling structure with dilated and transposed convolutional layers respectively with combinations between corresponding pooling and unpooling layers. Our network outperforms previous state of the art on semantic segmentation while still running at &gt;100 FPS which is important in the context of autonomous driving.</p><h3 name="5399" id="5399" class="graf graf--h3 graf-after--p">References</h3><p name="6d75" id="6d75" class="graf graf--p graf-after--h3">V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481–2495, 2017.</p><p name="88d3" id="88d3" class="graf graf--p graf-after--p">L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801–818, 2018.</p><p name="2002" id="2002" class="graf graf--p graf-after--p">S. Jégou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 11–19, 2017.</p><p name="1c09" id="1c09" class="graf graf--p graf-after--p">H. Li, P. Xiong, H. Fan, and J. Sun. Dfanet: Deep feature aggregation for real-time semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9522–9531, 2019.</p><h3 name="2cb9" id="2cb9" class="graf graf--h3 graf-after--p">Before You Go</h3><p name="289e" id="289e" class="graf graf--p graf-after--h3">Research Paper:<strong class="markup--strong markup--p-strong"> </strong><a href="https://arxiv.org/pdf/2007.12685" data-href="https://arxiv.org/pdf/2007.12685" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/2007.12685</a></p><p name="d75a" id="d75a" class="graf graf--p graf-after--p graf--trailing">Code:<strong class="markup--strong markup--p-strong"> </strong><a href="https://github.com/abhinavsagar/mssa" data-href="https://github.com/abhinavsagar/mssa" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/abhinavsagar/mssa</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@abhinav.sagar" class="p-author h-card">Abhinav Sagar</a> on <a href="https://medium.com/p/5442ac808b3e"><time class="dt-published" datetime="2020-07-27T18:13:14.491Z">July 27, 2020</time></a>.</p><p><a href="https://medium.com/@abhinav.sagar/semantic-segmentation-with-multi-scale-spatial-attention-5442ac808b3e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 28, 2021.</p></footer></article></body></html>