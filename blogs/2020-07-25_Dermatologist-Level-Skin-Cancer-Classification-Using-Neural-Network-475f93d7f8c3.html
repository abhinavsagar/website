<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Dermatologist Level Skin Cancer Classification Using Neural Network</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Dermatologist Level Skin Cancer Classification Using Neural Network</h1>
</header>
<section data-field="subtitle" class="p-summary">
To reduce false positives and false negatives
</section>
<section data-field="body" class="e-content">
<section name="f0fb" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b149" id="b149" class="graf graf--h3 graf--leading graf--title">Dermatologist Level Skin Cancer Classification Using Neural Network</h3><h4 name="714a" id="714a" class="graf graf--h4 graf-after--h3 graf--subtitle">To reduce false positives and false negatives</h4></div><div class="section-inner sectionLayout--fullWidth"><figure name="7955" id="7955" class="graf graf--figure graf--layoutFillWidth graf-after--h4"><img class="graf-image" data-image-id="1*vGlOh9zTgx96VMJpfnwjAQ.jpeg" data-width="6000" data-height="4000" src="https://cdn-images-1.medium.com/max/2560/1*vGlOh9zTgx96VMJpfnwjAQ.jpeg"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@pawel_czerwinski?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/@pawel_czerwinski?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Paweł Czerwiński</a> on <a href="https://unsplash.com/s/photos/cell?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/s/photos/cell?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="245b" id="245b" class="graf graf--p graf-after--figure">In this blog, we address the problem of skin cancer classification using convolutional neural networks. A lot of cancer cases early on are misdiagnosed leading to severe consequences including the death of patient. In this work, we address the above problems using deep neural networks and transfer learning architecture. We used publicly available ISIC databases for both training and testing our network. Our model achieves an accuracy of 0.935, precision 0.94, recall 0.77, F1 score 0.85 and ROC- AUC 0.861 which is better than the previous state of the art approaches.</p><h3 name="214a" id="214a" class="graf graf--h3 graf-after--p">Introduction</h3><p name="46fe" id="46fe" class="graf graf--p graf-after--h3">Skin cancer is the most widespread cancer diagnosed in the world. It is seen that if it can be diagnosed in its early phases, with choosing the appropriate treatment, survival rates are very good. Hence it is absolutely necessary to get to know at the earliest whether the symptoms of the patient correspond to cancer or not. Traditionally, doctors have been using their naked eye for skin cancer detection. Even experts have a tough time saying it, especially when the cancer is at the very early stages. This is where computer vision can help in automating the whole process. This automation could not only have a higher efficiency in avoiding both false positives and false negatives but also reduce time and manual work. Our model can be deployed on places which lacks expert doctors.</p><h3 name="277b" id="277b" class="graf graf--h3 graf-after--p">Dataset</h3><p name="b037" id="b037" class="graf graf--p graf-after--h3">We obtained a public dataset from ISIC website for skin cancer classification. We used 3000 images for training and 600 images for validation of size 224 × 224. The images are distributed equally between training and validation sets which are shown below in Fig 1.</p><figure name="f81f" id="f81f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L8FSnjC77E8tioEU7aKjQQ.png" data-width="881" data-height="804" src="https://cdn-images-1.medium.com/max/800/1*L8FSnjC77E8tioEU7aKjQQ.png"><figcaption class="imageCaption">Figure 1: Benign vs malignant images (Source: <a href="https://www.isic-archive.com/" data-href="https://www.isic-archive.com/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.isic-archive.com/</a>)</figcaption></figure><h3 name="2d68" id="2d68" class="graf graf--h3 graf-after--figure">Proposed Method</h3><p name="9d06" id="9d06" class="graf graf--p graf-after--h3">We used the concept of transfer learning for the classification. With transfer learning, instead of starting the learning process from scratch, the model starts from patterns that have been learned when solving a different problem. This way the model leverages previous learning and avoids starting from scratch. In image classification, transfer learning is usually expressed through the use of pre-trained models. A pre-trained model is a model that was trained on a large benchmark dataset to solve a problem similar to the one that we want to solve. We used five pre-trained models- Inception v3, InceptionResNet v2 and ResNet50, MobileNet and DenseNet169 as the pre trained weights for our work.</p><h3 name="127d" id="127d" class="graf graf--h3 graf-after--p">Inception V3</h3><p name="ba5f" id="ba5f" class="graf graf--p graf-after--h3">Google’s Inception v3 architecture was re-trained on our dataset by fine-tuning across all layers and replacing top layers with one average pooling, two fully connected and finally the softmax layer allowing to classify 2 diagnostic categories.</p><h3 name="eb0e" id="eb0e" class="graf graf--h3 graf-after--p">InceptionResNet v2</h3><p name="fab6" id="fab6" class="graf graf--p graf-after--h3">InceptionResNet v2 architecture was re-trained on our dataset by fine-tuning across all layers and replacing top layers with one global average pooling, one fully connected and finally the softmax layer allowing to classify 2 diagnostic categories.</p><h3 name="44d1" id="44d1" class="graf graf--h3 graf-after--p">ResNet50</h3><p name="ddc2" id="ddc2" class="graf graf--p graf-after--h3">It uses identity mapping to map the inputs. This identity mapping does not have any parameters and is just there to add the output from the previous layer to the layer ahead. The Skip Connections between layers add the outputs from previous layers to the outputs of stacked layers. This results in the ability to train much deeper networks than what was previously possible.</p><h3 name="6ca4" id="6ca4" class="graf graf--h3 graf-after--p">MobileNet</h3><p name="ae2a" id="ae2a" class="graf graf--p graf-after--h3">The fundamental part of MobileNet is depthwise separable filters, named as Depthwise Separable Convolution. These convolutions layers which is a form of factorized convolutional factorize a standard convolution into a depthwise convolution called a pointwise convolution.</p><h3 name="eed3" id="eed3" class="graf graf--h3 graf-after--p">DenseNet169</h3><p name="d4d9" id="d4d9" class="graf graf--p graf-after--h3">To solve the vanishing gradient problem, this architecture uses a simple connectivity pattern to ensure the maximum flow of information between layers both in forward and backward computation. The layers are connected in a way such that inputs from all preceding layers passes through its own feature-maps to all subsequent layers.</p><h3 name="8e50" id="8e50" class="graf graf--h3 graf-after--p">Network Architecture</h3><p name="d12a" id="d12a" class="graf graf--p graf-after--h3">The network architecture can be explained in the below points:</p><ol class="postList"><li name="c672" id="c672" class="graf graf--li graf-after--p">We split the dataset into two parts-training set and test set with 80 percent and 20 percent images respectively.</li><li name="23b4" id="23b4" class="graf graf--li graf-after--li">We used data augmentation like shearing, zooming, flipping and brightness change to increase the dataset size to more than double the original dataset size.</li><li name="907d" id="907d" class="graf graf--li graf-after--li">We tried with pre trained models like Inception v3, InceptionResNet v2, ResNet 50, MobileNet and DenseNet169 by fine tuning the last few layers of the network.</li><li name="1c4b" id="1c4b" class="graf graf--li graf-after--li">We used 50 percent dropout and batch normalization layers in between to reduce overfitting.</li><li name="716b" id="716b" class="graf graf--li graf-after--li">We used two dense layers at last with 64 neurons and 2 neurons respectively.</li><li name="2fa9" id="2fa9" class="graf graf--li graf-after--li">The last layer is used for the classification with softmax as the activation function.</li><li name="be9a" id="be9a" class="graf graf--li graf-after--li">We used binary cross entropy as the loss function.</li><li name="dea2" id="dea2" class="graf graf--li graf-after--li">ReLU was used throughout as the activation function except the last layer.</li></ol><h3 name="888d" id="888d" class="graf graf--h3 graf-after--li">Experimental Results</h3><p name="21c0" id="21c0" class="graf graf--p graf-after--h3">The loss vs epochs and accuracy vs epochs plot is shown in Figure 2:</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="7f1c" id="7f1c" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 49.141%;"><img class="graf-image" data-image-id="1*A530zY-EA9E8xOFT2ylPLw.png" data-width="375" data-height="252" src="https://cdn-images-1.medium.com/max/600/1*A530zY-EA9E8xOFT2ylPLw.png"></figure><figure name="5ed0" id="5ed0" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 50.859%;"><img class="graf-image" data-image-id="1*U61NkIlmUuQu6VVRqBlsng.png" data-width="388" data-height="252" src="https://cdn-images-1.medium.com/max/800/1*U61NkIlmUuQu6VVRqBlsng.png"><figcaption class="imageCaption" style="width: 196.622%; left: -96.622%;">Figure 2: a) Loss vs epoch b) Accuracy vs epoch</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="9e12" id="9e12" class="graf graf--p graf-after--figure">The confusion matrix for the classifier is shown in Figure 3:</p><figure name="8048" id="8048" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*JfjuAhIuA3jd1N-WLCHcTA.png" data-width="322" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*JfjuAhIuA3jd1N-WLCHcTA.png"><figcaption class="imageCaption">Figure 3: Confusion Matrix</figcaption></figure><h3 name="55b3" id="55b3" class="graf graf--h3 graf-after--figure">ROC-AUC</h3><p name="ffaa" id="ffaa" class="graf graf--p graf-after--h3">The ROC curve is calculated by plotting the sensitivity against 1-specificity and can be used to evaluate the classifier. The further the ROC curve deviates from the diagonal, the better the classifier. The ROC-AUC plot for the classifier is shown in Figure 4:</p><figure name="a469" id="a469" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*j0fncHRn91xfAd4tAF7ihg.png" data-width="389" data-height="278" src="https://cdn-images-1.medium.com/max/800/1*j0fncHRn91xfAd4tAF7ihg.png"><figcaption class="imageCaption">Figure 4: ROC-AUC plot</figcaption></figure><p name="ed84" id="ed84" class="graf graf--p graf-after--figure">The comparison of pre trained weights on results is shown in Table 5.</p><figure name="fbcf" id="fbcf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1dBBJHuTYJAtICdUezKKpg.png" data-width="814" data-height="260" src="https://cdn-images-1.medium.com/max/800/1*1dBBJHuTYJAtICdUezKKpg.png"></figure><p name="1638" id="1638" class="graf graf--p graf-after--figure">The comparison with previous state of the art results is shown in Table 6.</p><figure name="4c4d" id="4c4d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*24j6aLzFz_c8OgYWB1DRpg.png" data-width="726" data-height="354" src="https://cdn-images-1.medium.com/max/800/1*24j6aLzFz_c8OgYWB1DRpg.png"></figure><h3 name="16ed" id="16ed" class="graf graf--h3 graf-after--figure">Conclusions</h3><p name="2c84" id="2c84" class="graf graf--p graf-after--h3">In conclusion, this study investigated the ability of deep convolutional neural networks in the classification of benign vs malignant skin cancer. Our results show that state-of-the-art deep learning architectures trained on dermoscopy images (3600 in total composed of 3000 training and 600 validation) outperforms dermatologists. We showed with use of very deep convolutional neural networks using transfer learning and fine-tuning them on dermoscopy images, better diagnostic accuracy can be achieved compared to expert physicians and clinicians. These models can be easily implemented in dermoscopy systems or even on smartphones in order to assist dermatologists.</p><h3 name="5464" id="5464" class="graf graf--h3 graf-after--p">References</h3><p name="8327" id="8327" class="graf graf--p graf-after--h3"><a href="https://www.isic-archive.com/" data-href="https://www.isic-archive.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://www.isic-archive.com</a></p><h3 name="ae43" id="ae43" class="graf graf--h3 graf-after--p">Before You Go</h3><p name="b334" id="b334" class="graf graf--p graf-after--h3 graf--trailing">The corresponding link to <a href="https://abhinavsagar.github.io/files/skin_cnn.pdf" data-href="https://abhinavsagar.github.io/files/skin_cnn.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Paper</a> and <a href="https://github.com/abhinavsagar/skin-cancer" data-href="https://github.com/abhinavsagar/skin-cancer" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Code</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@abhinav.sagar" class="p-author h-card">Abhinav Sagar</a> on <a href="https://medium.com/p/475f93d7f8c3"><time class="dt-published" datetime="2020-07-25T20:05:24.961Z">July 25, 2020</time></a>.</p><p><a href="https://medium.com/@abhinav.sagar/dermatologist-level-skin-cancer-classification-using-neural-network-475f93d7f8c3" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 28, 2021.</p></footer></article></body></html>