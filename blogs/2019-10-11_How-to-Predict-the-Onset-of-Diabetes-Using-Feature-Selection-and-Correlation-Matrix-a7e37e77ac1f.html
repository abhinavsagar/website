<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How to Predict the Onset of Diabetes Using Feature Selection and Correlation Matrix</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How to Predict the Onset of Diabetes Using Feature Selection and Correlation Matrix</h1>
</header>
<section data-field="body" class="e-content">
<section name="4f99" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--fullWidth"><figure name="4454" id="4454" class="graf graf--figure graf--layoutFillWidth graf--leading"><img class="graf-image" data-image-id="1*4qygmZNHMG6pyqJ_NteQ5A.jpeg" data-width="5616" data-height="3744" src="https://cdn-images-1.medium.com/max/2560/1*4qygmZNHMG6pyqJ_NteQ5A.jpeg"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@sharonmccutcheon?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/@sharonmccutcheon?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Sharon McCutcheon</a> on <a href="https://unsplash.com/s/photos/sugar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/s/photos/sugar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><h3 name="4753" id="4753" class="graf graf--h3 graf-after--figure graf--title">How to Predict the Onset of Diabetes Using Feature Selection and Correlation Matrix</h3><h4 name="337c" id="337c" class="graf graf--h4 graf-after--h3 graf--subtitle">To improve accuracy, avoid overfitting, train the model faster and reduce the complexity of the model</h4><p name="e185" id="e185" class="graf graf--p graf-after--h4">Stuck behind the paywall? Click <a href="https://medium.com/p/how-to-predict-the-onset-of-diabetes-using-feature-selection-and-correlation-matrix-a7e37e77ac1f?source=email-c3f5233f3441--writer.postDistributed&amp;sk=fb907a36cf71530eb49f495ff5bcc030" data-href="https://medium.com/p/how-to-predict-the-onset-of-diabetes-using-feature-selection-and-correlation-matrix-a7e37e77ac1f?source=email-c3f5233f3441--writer.postDistributed&amp;sk=fb907a36cf71530eb49f495ff5bcc030" class="markup--anchor markup--p-anchor" target="_blank">here</a> to read the full story with my Friend Link!</p><p name="7e93" id="7e93" class="graf graf--p graf-after--p">In a high dimensional dataset, there are some entirely irrelevant and unimportant features.The contribution of these types of features is negligible towards predictive modeling as compared to the important features. They may have zero contribution as well. These features cause a number of problems which in turn prevents the process of efficient predictive modeling. Some of the problems associated are:</p><ol class="postList"><li name="0733" id="0733" class="graf graf--li graf-after--p">Unnecessary time and memory consumption while training the model.</li><li name="5819" id="5819" class="graf graf--li graf-after--li">These features act as a noise for which the machine learning model can perform poorly. The model can learn the noise ie irrelevant features in the dataset and thus overfit.</li><li name="862c" id="862c" class="graf graf--li graf-after--li">It reduced accuracy of the predictive model.</li></ol><p name="8dd0" id="8dd0" class="graf graf--p graf-after--li">So, what’s the solution here? The best solution is Feature Selection.</p><p name="40f3" id="40f3" class="graf graf--p graf-after--p">Feature Selection is the process of selecting out the most significant features from a given dataset. In many of the cases, Feature Selection can enhance the performance of a machine learning model as well.</p><figure name="a93f" id="a93f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nyC9bOwnK16Cl8jFYTyEFQ.png" data-width="297" data-height="169" src="https://cdn-images-1.medium.com/max/800/1*nyC9bOwnK16Cl8jFYTyEFQ.png"><figcaption class="imageCaption">Feature Selection demonstration</figcaption></figure><p name="0395" id="0395" class="graf graf--p graf-after--figure">The importance of feature selection:</p><ol class="postList"><li name="71b9" id="71b9" class="graf graf--li graf-after--p">It enables the machine learning algorithm to train faster.</li><li name="7419" id="7419" class="graf graf--li graf-after--li">It reduces the complexity of a model and makes it easier to interpret.</li><li name="272b" id="272b" class="graf graf--li graf-after--li">It improves the accuracy of a model if the right subset is chosen.</li><li name="ee9c" id="ee9c" class="graf graf--li graf-after--li graf--trailing">It reduces Overfitting.</li></ol></div></div></section><section name="4289" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3b5f" id="3b5f" class="graf graf--p graf--leading">In this article, I will demonstrate how feature selection can be used to reduce insignificant and unimportant features. I will be showing how feature importance and correlation matrix can be used for improving the machine learning model. I have used <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" data-href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pima Indians Diabetes Dataset</a> for this project.</p><h3 name="4847" id="4847" class="graf graf--h3 graf-after--p">The Challenge</h3><p name="a3ed" id="a3ed" class="graf graf--p graf-after--h3">To diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset.</p><h3 name="cb21" id="cb21" class="graf graf--h3 graf-after--p">Environment and tools</h3><ol class="postList"><li name="459c" id="459c" class="graf graf--li graf-after--h3">scikit-learn</li><li name="90c9" id="90c9" class="graf graf--li graf-after--li">seaborn</li><li name="f5a4" id="f5a4" class="graf graf--li graf-after--li">numpy</li><li name="2b9e" id="2b9e" class="graf graf--li graf-after--li">pandas</li><li name="656f" id="656f" class="graf graf--li graf-after--li">matplotlib</li></ol><h3 name="fa34" id="fa34" class="graf graf--h3 graf-after--li">Data</h3><p name="b1e1" id="b1e1" class="graf graf--p graf-after--h3">The dataset can be downloaded from the kaggle website which can be found <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" data-href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="2a86" id="2a86" class="graf graf--p graf-after--p">Description of variables in the dataset:</p><ul class="postList"><li name="cfcb" id="cfcb" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Pregnancies:</strong> Number of times pregnant</li><li name="6d06" id="6d06" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Glucose:</strong> Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li><li name="5640" id="5640" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">BloodPressure:</strong> Diastolic blood pressure (mm Hg)</li><li name="428c" id="428c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">SkinThickness:</strong> Triceps skin fold thickness (mm)</li><li name="7e82" id="7e82" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Insulin:</strong> 2-Hour serum insulin (mu U/ml)</li><li name="b958" id="b958" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">BMI:</strong> Body mass index (weight in kg/(height in m)²)</li><li name="f0cb" id="f0cb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">DiabetesPedigreeFunction:</strong> Diabetes pedigree function</li><li name="4982" id="4982" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Age:</strong> Age (years)</li><li name="d37d" id="d37d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Outcome:</strong> Class variable (0 or 1)</li></ul><h3 name="42c8" id="42c8" class="graf graf--h3 graf-after--li">Where is the code?</h3><p name="9bba" id="9bba" class="graf graf--p graf-after--h3">Without much ado, let’s get started with the code. The complete project on github can be found <a href="https://github.com/abhinavsagar/Machine-learning-tutorials/tree/master/diabetes%20features" data-href="https://github.com/abhinavsagar/Machine-learning-tutorials/tree/master/diabetes%20features" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="acc1" id="acc1" class="graf graf--p graf-after--p">I started with loading all the libraries and dependencies required.</p><pre name="1e4e" id="1e4e" class="graf graf--pre graf-after--p">import pandas as pd<br>import numpy as np<br>from sklearn.feature_selection import SelectKBest<br>from sklearn.feature_selection import chi2<br>from sklearn.ensemble import RandomForestClassifier<br>import matplotlib.pyplot as plt<br>import seaborn as sns</pre><p name="3d83" id="3d83" class="graf graf--p graf-after--pre">Let’s see how the dataset looks like.</p><p name="a4c3" id="a4c3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">read_csv</strong> is a pandas function to read csv files and do operations on it later. <strong class="markup--strong markup--p-strong">head()</strong> method is used to return top n (5 by default) rows of a DataFrame.</p><pre name="312b" id="312b" class="graf graf--pre graf-after--p">df = pd.read_csv(&quot;../input/diabetes.csv&quot;)<br>df.head()</pre><figure name="1930" id="1930" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*VDE_GTFC2YUEv_VM9mHaqw.png" data-width="732" data-height="171" src="https://cdn-images-1.medium.com/max/800/1*VDE_GTFC2YUEv_VM9mHaqw.png"></figure><p name="a261" id="a261" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">shape()</strong> method is used to return the number of rows and columns present in the DataFrame.</p><pre name="176a" id="176a" class="graf graf--pre graf-after--p">df.shape</pre><figure name="62ca" id="62ca" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*KHRiQnNNi54NdjHBfjBLpg.png" data-width="72" data-height="22" src="https://cdn-images-1.medium.com/max/800/1*KHRiQnNNi54NdjHBfjBLpg.png"></figure><p name="7fcd" id="7fcd" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">columns()</strong> method is used to return all the column names present in the DataFrame.</p><pre name="9a33" id="9a33" class="graf graf--pre graf-after--p">df.columns</pre><figure name="255c" id="255c" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*4qBVFhhWtPcbPFfuh35DBA.png" data-width="604" data-height="59" src="https://cdn-images-1.medium.com/max/800/1*4qBVFhhWtPcbPFfuh35DBA.png"></figure><p name="5496" id="5496" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">info()</strong> method is used to get a summary of the DataFrame. It is useful when doing exploratory data analysis (EDA) of the data.</p><pre name="f9c0" id="f9c0" class="graf graf--pre graf-after--p">df.info()</pre><figure name="a132" id="a132" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*vDqqN8dlRY9tL21vb4vvNw.png" data-width="404" data-height="254" src="https://cdn-images-1.medium.com/max/800/1*vDqqN8dlRY9tL21vb4vvNw.png"></figure><p name="096d" id="096d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">describe()</strong> method computes a summary of statistics like count, mean, standard deviation, min, max and quartile values pertaining to the DataFrame columns.</p><pre name="76b7" id="76b7" class="graf graf--pre graf-after--p">df.describe()</pre><figure name="7947" id="7947" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*jcIR2hS-jrG4BCr4AFdixg.png" data-width="894" data-height="248" src="https://cdn-images-1.medium.com/max/800/1*jcIR2hS-jrG4BCr4AFdixg.png"></figure><p name="db74" id="db74" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">sum()</strong> method of the DataFrame returned by <strong class="markup--strong markup--p-strong">isnull()</strong> method will give a series containing data about count of NaN in each column.</p><pre name="bf13" id="bf13" class="graf graf--pre graf-after--p">df.isnull().sum()</pre><figure name="d722" id="d722" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*qdIJJ5D8tWXUmwEZgcXvUw.png" data-width="245" data-height="180" src="https://cdn-images-1.medium.com/max/800/1*qdIJJ5D8tWXUmwEZgcXvUw.png"></figure><p name="9704" id="9704" class="graf graf--p graf-after--figure">There are no null values in the dataset. Hence no data cleaning is required. Let’s continue.</p><p name="f088" id="f088" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">corr() </strong>method is used to find the pairwise correlation of all the columns in the DataFrame.</p><pre name="eec5" id="eec5" class="graf graf--pre graf-after--p">df.corr()</pre><figure name="660f" id="660f" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*RIuH-188Dahy03PKP8hckw.png" data-width="936" data-height="268" src="https://cdn-images-1.medium.com/max/800/1*RIuH-188Dahy03PKP8hckw.png"></figure><p name="7d8f" id="7d8f" class="graf graf--p graf-after--figure">Statistical tests can be used to select those features that have the strongest relationship with the output variable.</p><p name="af9a" id="af9a" class="graf graf--p graf-after--p">The below code is used to get the chi-squared statistical test to select all the features from the dataset. Also it assigns a score for every feature depending on the chi-squared test value.</p><pre name="55c3" id="55c3" class="graf graf--pre graf-after--p">X = df.iloc[:,0:8] <br>y = df.iloc[:,-1]   <br>bestfeatures = SelectKBest(score_func=chi2, k=8)<br>fit = bestfeatures.fit(X,y)<br>dfscores = pd.DataFrame(fit.scores_)<br>dfcolumns = pd.DataFrame(X.columns) <br>featureScores = pd.concat([dfcolumns,dfscores],axis=1)<br>featureScores.columns = [&#39;Features&#39;,&#39;Score&#39;]  <br>print(featureScores.nlargest(8,&#39;Score&#39;))</pre><figure name="9f9a" id="9f9a" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*Bwq-Ie_RZh7e5hvfKJHJ8g.png" data-width="350" data-height="157" src="https://cdn-images-1.medium.com/max/800/1*Bwq-Ie_RZh7e5hvfKJHJ8g.png"></figure><p name="7155" id="7155" class="graf graf--p graf-after--figure">Insulin has the highest score followed by glucose. This means that these features are more important for diabetes prediction than others.</p><p name="b36d" id="b36d" class="graf graf--p graf-after--p">Next, I compared feature importance using bar plots. Feature importance gives you a score for each feature present in the data. The higher the score, the more important or relevant is that feature towards the output variable. I have used Random Forest Classifier for ranking all the 8 features present in the dataset.</p><pre name="637f" id="637f" class="graf graf--pre graf-after--p">model = RandomForestClassifier()<br>model.fit(X,y)<br>print(model.feature_importances_) <br>feat_importances = pd.Series(model.feature_importances_, index=X.columns)<br>feat_importances.nlargest(8).plot(kind=&#39;barh&#39;)<br>plt.show()</pre><figure name="38d1" id="38d1" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*yfX95BBDPnnDH-u8zO2L1g.png" data-width="534" data-height="259" src="https://cdn-images-1.medium.com/max/800/1*yfX95BBDPnnDH-u8zO2L1g.png"></figure><p name="6bc1" id="6bc1" class="graf graf--p graf-after--figure">Glucose has the highest feature importance followed by body mass index. It is strange to see that insulin ranks much lower in this case.</p><p name="1a53" id="1a53" class="graf graf--p graf-after--p">Finally I made a correlation matrix to show how the features are related to each other or the target variable.Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)</p><pre name="9edf" id="9edf" class="graf graf--pre graf-after--p">corrmat = df.corr()<br>top_corr_features = corrmat.index<br>plt.figure(figsize=(10,10))<br>g=sns.heatmap(df[top_corr_features].corr(),annot=<strong class="markup--strong markup--pre-strong">True</strong>,cmap=&quot;RdYlGn&quot;)</pre><figure name="5737" id="5737" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*E-9bGUH0rfW9aIom0deCBA.png" data-width="646" data-height="618" src="https://cdn-images-1.medium.com/max/800/1*E-9bGUH0rfW9aIom0deCBA.png"></figure><h3 name="be32" id="be32" class="graf graf--h3 graf-after--figure">Conclusions</h3><p name="ab67" id="ab67" class="graf graf--p graf-after--h3">In this article, I started with the initial part of the data science workflow ie data exploration and data preparation. I continued with demonstrating how feature selection can be used to reduce insignificant and unimportant features by comparing the importance of features. Finally I concluded with making a correlation matrix for showing correlation coefficients among all the features.</p><h3 name="c767" id="c767" class="graf graf--h3 graf-after--p">References/Further Readings</h3><div name="c8b0" id="c8b0" class="graf graf--mixtapeEmbed graf-after--h3"><a href="https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8" data-href="https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8"><strong class="markup--strong markup--mixtapeEmbed-strong">Machine Learning Workflow on Diabetes Data : Part 01</strong><br><em class="markup--em markup--mixtapeEmbed-em">“Machine learning in a medical setting can help enhance medical diagnosis dramatically.”</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="9ee99b195c6a3545f1ceb25b8c9d960e" data-thumbnail-img-id="1*INSggrGiQ1lCgU8YTsfEVw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*INSggrGiQ1lCgU8YTsfEVw.png);"></a></div><div name="9b2e" id="9b2e" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/" data-href="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/"><strong class="markup--strong markup--mixtapeEmbed-strong">Feature Importance and Feature Selection With XGBoost in Python</strong><br><em class="markup--em markup--mixtapeEmbed-em">A benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide…</em>machinelearningmastery.com</a><a href="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f8bc40479b136371c3f78b9c61a3e1b1" data-thumbnail-img-id="0*Ma18DcxcCBgzCZtX" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Ma18DcxcCBgzCZtX);"></a></div><div name="3cb4" id="3cb4" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://analyticsindiamag.com/what-are-feature-selection-techniques-in-machine-learning/" data-href="https://analyticsindiamag.com/what-are-feature-selection-techniques-in-machine-learning/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://analyticsindiamag.com/what-are-feature-selection-techniques-in-machine-learning/"><strong class="markup--strong markup--mixtapeEmbed-strong">What Are Feature Selection Techniques In Machine Learning?</strong><br><em class="markup--em markup--mixtapeEmbed-em">Image for representation purpose Feature selection is the method of reducing data dimension while doing predictive…</em>analyticsindiamag.com</a><a href="https://analyticsindiamag.com/what-are-feature-selection-techniques-in-machine-learning/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="5a5eb945c0b7b75bf0093adb22bad02c" data-thumbnail-img-id="0*f7do0hnqkH-RcPIr" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*f7do0hnqkH-RcPIr);"></a></div><h3 name="e815" id="e815" class="graf graf--h3 graf-after--mixtapeEmbed">Before You Go</h3><p name="e21e" id="e21e" class="graf graf--p graf-after--h3">The corresponding source code can be found here.</p><div name="ebfa" id="ebfa" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/abhinavsagar/Machine-learning-tutorials/tree/master/diabetes%20features" data-href="https://github.com/abhinavsagar/Machine-learning-tutorials/tree/master/diabetes%20features" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/abhinavsagar/Machine-learning-tutorials/tree/master/diabetes%20features"><strong class="markup--strong markup--mixtapeEmbed-strong">abhinavsagar/Machine-learning-tutorials</strong><br><em class="markup--em markup--mixtapeEmbed-em">You can&#39;t perform that action at this time. You signed in with another tab or window. You signed out in another tab or…</em>github.com</a><a href="https://github.com/abhinavsagar/Machine-learning-tutorials/tree/master/diabetes%20features" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="2951e83292b47884b1c1a0922ac84352"></a></div><h3 name="d9d3" id="d9d3" class="graf graf--h3 graf-after--mixtapeEmbed">Contacts</h3><p name="4173" id="4173" class="graf graf--p graf-after--h3">If you want to keep updated with my latest articles and projects <a href="https://medium.com/@abhinav.sagar" data-href="https://medium.com/@abhinav.sagar" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">follow me on Medium</a>. These are some of my contacts details:</p><ul class="postList"><li name="9b92" id="9b92" class="graf graf--li graf-after--p"><a href="https://abhinavsagar.github.io/" data-href="https://abhinavsagar.github.io/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Personal Website</a></li><li name="00bb" id="00bb" class="graf graf--li graf-after--li"><a href="https://in.linkedin.com/in/abhinavsagar4" data-href="https://in.linkedin.com/in/abhinavsagar4" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Linkedin</a></li><li name="8da1" id="8da1" class="graf graf--li graf-after--li"><a href="https://medium.com/@abhinav.sagar" data-href="https://medium.com/@abhinav.sagar" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Medium Profile</a></li><li name="5f66" id="5f66" class="graf graf--li graf-after--li"><a href="https://github.com/abhinavsagar" data-href="https://github.com/abhinavsagar" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">GitHub</a></li><li name="3309" id="3309" class="graf graf--li graf-after--li"><a href="https://www.kaggle.com/abhinavsagar" data-href="https://www.kaggle.com/abhinavsagar" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Kaggle</a></li></ul><p name="7927" id="7927" class="graf graf--p graf-after--li graf--trailing">Happy reading, happy learning and happy coding!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@abhinav.sagar" class="p-author h-card">Abhinav Sagar</a> on <a href="https://medium.com/p/a7e37e77ac1f"><time class="dt-published" datetime="2019-10-11T10:08:38.238Z">October 11, 2019</time></a>.</p><p><a href="https://medium.com/@abhinav.sagar/how-to-predict-the-onset-of-diabetes-using-feature-selection-and-correlation-matrix-a7e37e77ac1f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 28, 2021.</p></footer></article></body></html>