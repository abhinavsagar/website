<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Stochastic Gradient Descent for Bayesian Neural Networks</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Stochastic Gradient Descent for Bayesian Neural Networks</h1>
</header>
<section data-field="subtitle" class="p-summary">
An alternative perspective on Evidence Lower Bound
</section>
<section data-field="body" class="e-content">
<section name="632e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1f13" id="1f13" class="graf graf--h3 graf--leading graf--title">Stochastic Gradient Descent for Bayesian Neural Networks</h3><h4 name="2b80" id="2b80" class="graf graf--h4 graf-after--h3 graf--subtitle">An alternative perspective on Evidence Lower Bound</h4></div><div class="section-inner sectionLayout--fullWidth"><figure name="320b" id="320b" class="graf graf--figure graf--layoutFillWidth graf-after--h4"><img class="graf-image" data-image-id="1*jYuoNcJOuwtVVBqnqaV0HQ.jpeg" data-width="7360" data-height="4912" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2560/1*jYuoNcJOuwtVVBqnqaV0HQ.jpeg"><figcaption class="imageCaption">TPhoto by <a href="https://unsplash.com/@aleskrivec?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/@aleskrivec?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Ales Krivec</a> on <a href="https://unsplash.com/s/photos/descent?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/s/photos/descent?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="ad1e" id="ad1e" class="graf graf--p graf-after--figure">This blog presents the research work done as part of Bachelors thesis at Vellore Institute of Technology.</p><h3 name="b98a" id="b98a" class="graf graf--h3 graf-after--p">Important Points:</h3><ol class="postList"><li name="e79b" id="e79b" class="graf graf--li graf-after--h3">Variational inference techniques is used to minimize the KL divergence between the variational distribution and unknown posterior distribution. This is done by maximizing the Evidence Lower Bound (ELBO).</li><li name="9c37" id="9c37" class="graf graf--li graf-after--li">We present a new approach in which a neural network is used to parametrize these distributions using Stochastic Gradient Descent.</li><li name="0e15" id="0e15" class="graf graf--li graf-after--li">We show how SGD can be applied on bayesian neural networks by gradient estimation techniques.</li><li name="095c" id="095c" class="graf graf--li graf-after--li">For evaluation, we tested our model on 5 UCI datasets and the metrics chosen are Root Mean Square Error (RMSE) error and negative log likelihood.</li><li name="466e" id="466e" class="graf graf--li graf-after--li">Our work outperforms previous state of the art approaches on regression benchmarks.</li></ol><h3 name="edec" id="edec" class="graf graf--h3 graf-after--li">Introduction</h3><p name="97fb" id="97fb" class="graf graf--p graf-after--h3">Recently, a surge of papers have been published on Bayesian Neural Networks. This shows that there has been a revival of interest in BNN’s whose history dates back to 1993. In this approach, rather than considering the parameters of the neural network as point estimates, we sample them as continuous distributions. It helps us infer the uncertainty involved while making the predictions. This is very important in sensitive domains where not only we want the predictions made by the model but also with how much certainty it is making those predictions.</p><p name="6678" id="6678" class="graf graf--p graf-after--p">The problem with this approach lies in the calculation of posterior distribution which is often intractable. Hence for the computation, it is necessary to convert the variational distribution into a tractable posterior distribution. Variational inference is used to convert the inference problem into an optimization problem with the objective of minimizing the KL-divergence between variational distribution and the true posterior. This is done by maximizing the ELBO.</p><h3 name="5bda" id="5bda" class="graf graf--h3 graf-after--p">Variational Inference</h3><p name="851d" id="851d" class="graf graf--p graf-after--h3">A probabilistic model is denoted using observations x, latent variables z and model parameters θ. The optimal θ value has to be found to maximize the marginal likelihood as defined in the equation below:</p><figure name="f0cd" id="f0cd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QQQO2oEZpnYFvwgS9RFeuw.png" data-width="346" data-height="95" src="https://cdn-images-1.medium.com/max/800/1*QQQO2oEZpnYFvwgS9RFeuw.png"></figure><p name="653c" id="653c" class="graf graf--p graf-after--figure">where we refer pθ(x|z) as the generative distribution and pθ(z) as the prior distribution.</p><p name="e4a4" id="e4a4" class="graf graf--p graf-after--p">Computing the posterior by doing inference is intractable. Hence we maximize Evidence Lower Bound (ELBO) in variational inference which can be computed by approximating posterior on the latent variable. The equation can be converted to an optimization problem as defined in the equation below:</p><figure name="311a" id="311a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sPQRNWJv-oNzEAIOKWN8EA.png" data-width="573" data-height="235" src="https://cdn-images-1.medium.com/max/800/1*sPQRNWJv-oNzEAIOKWN8EA.png"></figure><p name="6276" id="6276" class="graf graf--p graf-after--figure">However doing the computation still requires a lot of computational resources. Thus, variational inference techniques using ELBO is not scalable to the larger datasets.</p><h3 name="6f34" id="6f34" class="graf graf--h3 graf-after--p">BNN Using Stochastic Gradient Descent</h3><p name="e529" id="e529" class="graf graf--p graf-after--h3">We can optimize the variational parameters φ by using stochastic gradient descent with the following update rule as defined in the equation below:</p><figure name="514d" id="514d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WEV590h0N8gvJPF22mIijw.png" data-width="396" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*WEV590h0N8gvJPF22mIijw.png"></figure><p name="28a8" id="28a8" class="graf graf--p graf-after--figure">Here γ is learning rate and η is the size of randomly sampled mini batches of training data.</p><p name="0af9" id="0af9" class="graf graf--p graf-after--p">We define ESBO S() as an alternative lower bound having its posterior distribution unknown. Hence we use joint distribution instead. The weights are normalized to tackle the intractable integral present in the denominator. The dataset has been divided into mini-batches and the operations are carried on them in successive iterations. The ESBOS() is defined in the equation below:</p><figure name="227a" id="227a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vYMFpN45c_Vn2vnBZsjPLA.png" data-width="900" data-height="121" src="https://cdn-images-1.medium.com/max/800/1*vYMFpN45c_Vn2vnBZsjPLA.png"></figure><p name="4332" id="4332" class="graf graf--p graf-after--figure">Using the reparameterization trick, the gradient can be written as shown in the equation below:</p><figure name="ccc2" id="ccc2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*z-1qropd1s1me9u1LITvsg.png" data-width="1021" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*z-1qropd1s1me9u1LITvsg.png"></figure><p name="7ddb" id="7ddb" class="graf graf--p graf-after--figure">This operation is very helpful in reducing the complexity of variational inference models. Now Stochastic Gradient Descent (SGD) can be applied to minimize the ESBO S(). This operation is shown in the below equation:</p><figure name="ed13" id="ed13" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cnhE2BG4vLlMpT8DQGPbSQ.png" data-width="825" data-height="129" src="https://cdn-images-1.medium.com/max/800/1*cnhE2BG4vLlMpT8DQGPbSQ.png"></figure><h3 name="34f1" id="34f1" class="graf graf--h3 graf-after--figure">Algorithm</h3><p name="1304" id="1304" class="graf graf--p graf-after--h3">The algorithm used in this work is shown below:</p><figure name="308e" id="308e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9sw-P0JhcStWJskJmMg_pA.png" data-width="1090" data-height="354" src="https://cdn-images-1.medium.com/max/800/1*9sw-P0JhcStWJskJmMg_pA.png"></figure><h3 name="1c35" id="1c35" class="graf graf--h3 graf-after--figure">Experimental Details</h3><p name="af73" id="af73" class="graf graf--p graf-after--h3">The experimental details can be summarized in the below points:</p><ol class="postList"><li name="c9b8" id="c9b8" class="graf graf--li graf-after--p">For bayesian neural network regression, we used datasets from UCI repository: Boston, Concrete, Energy, Protein, Wine.</li><li name="ffe0" id="ffe0" class="graf graf--li graf-after--li">We used a neural network with one hidden layer with 50 neurons in each case.</li><li name="68b1" id="68b1" class="graf graf--li graf-after--li">We set (0, 1) as the prior distribution for the weight and bias of the neural network, ReLU as the activation function and batch size value as 32.</li><li name="6e8a" id="6e8a" class="graf graf--li graf-after--li">The datasets are randomly partitioned into 90 percent with training data and 10 percent for testing, and the results are averaged over 50 random trials.</li></ol><h3 name="f208" id="f208" class="graf graf--h3 graf-after--li">Results</h3><p name="2297" id="2297" class="graf graf--p graf-after--h3">The average RMSE loss and average log likelihood values obtained are shown in Table 1 and Table 2 respectively:</p><figure name="64ad" id="64ad" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CLkfOKCGFlc-VFj5NOqKMA.png" data-width="1105" data-height="287" src="https://cdn-images-1.medium.com/max/800/1*CLkfOKCGFlc-VFj5NOqKMA.png"></figure><figure name="93d4" id="93d4" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*wcR5Pn4zF9c78VcbGv3_ZQ.png" data-width="1146" data-height="287" src="https://cdn-images-1.medium.com/max/800/1*wcR5Pn4zF9c78VcbGv3_ZQ.png"></figure><h3 name="cb90" id="cb90" class="graf graf--h3 graf-after--figure">Conclusions</h3><p name="1733" id="1733" class="graf graf--p graf-after--h3">In this blog, we presented our research on how a Bayesian neural network can be trained using stochastic gradient descent techniques. We outlined the problem in traditional variational inference approaches and how the problem can be converted into a tractable one using an alternative lower bound ESBO. We presented our algorithm which used gradient estimation techniques for computing the inference. We evaluated our work on UCI datasets and our approach outperforms previous state of the art algorithms on regression benchmarks.</p><h3 name="4cbc" id="4cbc" class="graf graf--h3 graf-after--p">References</h3><p name="f13d" id="f13d" class="graf graf--p graf-after--h3">A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken elbo. In International Conference on Machine Learning, pages 159–168, 2018.</p><p name="90ed" id="90ed" class="graf graf--p graf-after--p">D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017.</p><p name="4d31" id="4d31" class="graf graf--p graf-after--p">P. Chaudhari and S. Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In 2018 Information Theory and Applications Workshop (ITA), pages 1–10. IEEE, 2018.</p><p name="1432" id="1432" class="graf graf--p graf-after--p">D. Flam-Shepherd, J. Requeima, and D. Duvenaud. Mapping gaussian process priors to bayesian neural networks. In NIPS Bayesian deep learning workshop, 2017.</p><p name="d85c" id="d85c" class="graf graf--p graf-after--p">M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, pages 1225–1234, 2016.</p><h3 name="a8a9" id="a8a9" class="graf graf--h3 graf-after--p">Before You Go</h3><p name="d172" id="d172" class="graf graf--p graf-after--h3">Research Paper:<strong class="markup--strong markup--p-strong"> </strong><a href="https://arxiv.org/pdf/2006.08453.pdf" data-href="https://arxiv.org/pdf/2006.08453.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/2006.08453.pdf</a></p><p name="4bfc" id="4bfc" class="graf graf--p graf-after--p graf--trailing">Code:<strong class="markup--strong markup--p-strong"> </strong><a href="https://github.com/abhinavsagar/bnn-sgd" data-href="https://github.com/abhinavsagar/bnn-sgd" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/abhinavsagar/bnn-sgd</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@abhinav.sagar" class="p-author h-card">Abhinav Sagar</a> on <a href="https://medium.com/p/90b783cea7fc"><time class="dt-published" datetime="2020-07-28T18:06:59.700Z">July 28, 2020</time></a>.</p><p><a href="https://medium.com/@abhinav.sagar/stochastic-gradient-descent-for-bayesian-neural-networks-90b783cea7fc" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 28, 2021.</p></footer></article></body></html>