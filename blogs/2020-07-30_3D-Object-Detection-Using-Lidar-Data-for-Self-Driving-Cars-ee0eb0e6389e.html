<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>3D Object Detection Using Lidar Data for Self Driving Cars</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">3D Object Detection Using Lidar Data for Self Driving Cars</h1>
</header>
<section data-field="subtitle" class="p-summary">
New state of the art neural network for 3D Object Detection
</section>
<section data-field="body" class="e-content">
<section name="fd46" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="093b" id="093b" class="graf graf--h3 graf--leading graf--title">3D Object Detection Using Lidar Data for Self Driving Cars</h3><h4 name="8611" id="8611" class="graf graf--h4 graf-after--h3 graf--subtitle">New state of the art neural network for 3D Object Detection</h4></div><div class="section-inner sectionLayout--fullWidth"><figure name="1147" id="1147" class="graf graf--figure graf--layoutFillWidth graf-after--h4"><img class="graf-image" data-image-id="1*0Lycczfg_b-yBSWbMb0ozg.jpeg" data-width="5472" data-height="3648" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2560/1*0Lycczfg_b-yBSWbMb0ozg.jpeg"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@introspectivedsgn?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/@introspectivedsgn?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Erik Mclean</a> on <a href="https://unsplash.com/s/photos/car-on-road?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/s/photos/car-on-road?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="44cb" id="44cb" class="graf graf--p graf-after--figure">In this blog, we present our research work on 3D Object Detection in real time using lidar data.</p><h3 name="78eb" id="78eb" class="graf graf--h3 graf-after--p">Important Points</h3><ol class="postList"><li name="720a" id="720a" class="graf graf--li graf-after--h3">A novel neural network architecture is used to simultaneously detect and regress bounding box over all the objects present in the image.</li><li name="ecd8" id="ecd8" class="graf graf--li graf-after--li">We used 2D Bird’s Eye View in place of 3D voxel grid data because it is much less computationally heavy. This will also make our detector to be easily deployed to real work settings especially in case of self driving cars.</li><li name="0d08" id="0d08" class="graf graf--li graf-after--li">We compare the results with different backbone architectures including the standard ones like VGG, ResNet, Inception with our backbone.</li><li name="5bbb" id="5bbb" class="graf graf--li graf-after--li">We show the optimization and ablation studies including designing an efficient anchor.</li><li name="b4f3" id="b4f3" class="graf graf--li graf-after--li">We use the Kitti 3D Bird’s Eye View dataset for benchmarking and evaluating our results.</li><li name="2000" id="2000" class="graf graf--li graf-after--li">Our work surpasses the previous state of the art both in terms of average precision while still running at &gt; 30 FPS.</li></ol><h3 name="150f" id="150f" class="graf graf--h3 graf-after--li">2D Object Detection</h3><p name="b019" id="b019" class="graf graf--p graf-after--h3">The 2D object detection algorithms can be broadly grouped into the following two types:</p><ol class="postList"><li name="ff48" id="ff48" class="graf graf--li graf-after--p">Single stage detector — Yolo and SSD.</li><li name="d935" id="d935" class="graf graf--li graf-after--li">Two stage detector — RCNN, Fast RCNN and Faster RCNN.</li></ol><p name="79ba" id="79ba" class="graf graf--p graf-after--li">The difference between the two is that in the two stage detectors, the first stage uses region proposal networks to generate regions of interest and the second stage uses these regions of interest for object classification and bounding box regression. On the other hand, a single stage detector uses the input image to directly learn the class wise probability and bounding box coordinates. Thus these architectures treat the object detection as a simple regression problem and thus are faster but less accurate.</p><h3 name="e979" id="e979" class="graf graf--h3 graf-after--p">3D Object Detection</h3><p name="b8c2" id="b8c2" class="graf graf--p graf-after--h3">Both camera based and lidar based approaches have been used in literature. Lidar data has been proven to be a better alternative achieving higher accuracy than camera based approaches. The challenge with using lidar data is that it produces data in the form of point clouds which have millions of points thus increasing the computational cost and processing time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference.</p><h3 name="22f4" id="22f4" class="graf graf--h3 graf-after--p">Dataset</h3><p name="e64a" id="e64a" class="graf graf--p graf-after--h3">We used the Kitti dataset which contains LIDAR data taken from a sensor mounted in front of the car. Since the data contains millions of points and is of quite high resolution, processing is a challenge especially in real world situations. The task is to detect and regress a bounding box for 3D objects detected in real time. The dataset has 7481 training images and 7518 test point clouds comprising a total of labelled objects. The 3D object KITTI benchmark provides 3D bounding boxes for object classes including cars, vans, trucks, pedestrians and cyclists which are labelled manually in 3D point clouds on the basis of information from the camera.</p><h3 name="a4de" id="a4de" class="graf graf--h3 graf-after--p">Network Architecture</h3><p name="b000" id="b000" class="graf graf--p graf-after--h3">Our network architecture can be explained in the following points:</p><ol class="postList"><li name="8928" id="8928" class="graf graf--li graf-after--p">We divided the point cloud data into 3D voxel grid cells.</li><li name="898a" id="898a" class="graf graf--li graf-after--li">Our CNN backbone takes as input the image in the form of voxel and outputs a feature vector.</li><li name="e6b1" id="e6b1" class="graf graf--li graf-after--li">Multiple residual blocks are used for feature extraction with skip connections in between connecting two adjacent blocks.</li><li name="c3bb" id="c3bb" class="graf graf--li graf-after--li">These blocks are connected to upsampling blocks each except the first two residual blocks.</li><li name="ac4b" id="ac4b" class="graf graf--li graf-after--li">The last upsampling block is connected to two header network blocks which are further connected to two separator blocks that uses a bounding box regressor.</li><li name="1e5d" id="1e5d" class="graf graf--li graf-after--li">Anchors are used in these header blocks to adjust the coordinates according to the size and shape of the body detected.</li><li name="7b9f" id="7b9f" class="graf graf--li graf-after--li">A point wise concatenation operator is used which concatenates each point wise feature vector with the locally aggregated features.</li></ol><p name="4de2" id="4de2" class="graf graf--p graf-after--li">Each of the residual blocks are made up of: a fully connected layer followed by a non linearity activation function which is ReLu used in this case and a batch normalization layer. The network architecture is shown in Fig 1:</p><figure name="6d57" id="6d57" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*R8eQeAD1L72m7dCdZceCyQ.png" data-width="648" data-height="486" src="https://cdn-images-1.medium.com/max/800/1*R8eQeAD1L72m7dCdZceCyQ.png"><figcaption class="imageCaption">Figure 1: Our network architecture</figcaption></figure><h3 name="7c4c" id="7c4c" class="graf graf--h3 graf-after--figure">Anchors</h3><p name="bc3e" id="bc3e" class="graf graf--p graf-after--h3">Anchors are very important for efficient object detection. These are basically prior beliefs containing information of the size for the detected object, its position in the image, its pose, its orientation etc. Anchors of multiple shape and size are more stable while also helping in reducing the computational burden and time taken by the model.</p><h3 name="000b" id="000b" class="graf graf--h3 graf-after--p">Loss Functions</h3><p name="9d9f" id="9d9f" class="graf graf--p graf-after--h3">A vector s = (x, y, z, l, h, w, θ) represents 3D bounding box center coordinates, height, width, length and yaw respectively. The geometric relations between various parameters is illustrated where s represents the ground truth vector and a represents the anchor vector. The localization regression between ground truth and anchors is defined in Equation 1:</p><figure name="8714" id="8714" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yLEkpH5ZNqm9APZ5543AjQ.png" data-width="990" data-height="180" src="https://cdn-images-1.medium.com/max/800/1*yLEkpH5ZNqm9APZ5543AjQ.png"></figure><p name="9f85" id="9f85" class="graf graf--p graf-after--figure">Since the angle localization loss cannot distinguish the bounding boxes which are flipped, we use a softmax classification loss as shown for both positive and negative anchors. For the object classification, we used focal loss as shown in Equation 2 and Equation 3 respectively:</p><figure name="e5b6" id="e5b6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*OdHvEdLEE0B3Xf12hKNRHw.png" data-width="941" data-height="178" src="https://cdn-images-1.medium.com/max/800/1*OdHvEdLEE0B3Xf12hKNRHw.png"></figure><p name="207b" id="207b" class="graf graf--p graf-after--figure">We used binary cross entropy loss for detection and a variant of huber loss for regression. Let i and j denote the positive and negative anchors and let p denote the sigmoid activation for the classification network. Let pos represent the positive regression anchors and neg the negative regression anchors. The overall loss function used is shown in Equation 4:</p><figure name="fa39" id="fa39" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*cJWBuIb3v5IQ1qVvoWutRw.png" data-width="1136" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*cJWBuIb3v5IQ1qVvoWutRw.png"></figure><p name="faf9" id="faf9" class="graf graf--p graf-after--figure">Here α, β and γ are the hyperparameters which we tuned using bayesian optimization. The optimal value of α, β and γ comes out to be 0.36, 0.14 and 0.63 respectively.</p><h3 name="50a0" id="50a0" class="graf graf--h3 graf-after--p">Results</h3><p name="f9c9" id="f9c9" class="graf graf--p graf-after--h3">Table 6 compares the results of the LIDAR based 3D object detector on KITTI testing set. For the fair comparison VeloFCN, 3DFCN, MV3D architectures have been shown with their inference time, class wise average precision metric for all the three categories including easy, medium and hard.</p><figure name="7fcd" id="7fcd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rgffIIysHadNtcJHxOD3EA.png" data-width="1088" data-height="284" src="https://cdn-images-1.medium.com/max/800/1*rgffIIysHadNtcJHxOD3EA.png"></figure><p name="069a" id="069a" class="graf graf--p graf-after--figure">Table 7. compares our result with F-PointNet, AVOD and VoxelNet architectures. FPS along with class wise average precision is used for all three cases i.e. easy, medium and hard. All the three classes here have been compared against i.e. car, pedestrian and cyclist.</p><figure name="f003" id="f003" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QEGQMzUlt2-De5XtFEKIeA.png" data-width="1326" data-height="282" src="https://cdn-images-1.medium.com/max/800/1*QEGQMzUlt2-De5XtFEKIeA.png"></figure><p name="4786" id="4786" class="graf graf--p graf-after--figure">The performance comparison of 3D object detection on the KITTI 3D object detection benchmark and BEV benchmark for the class car is shown in Table 8.</p><figure name="5c06" id="5c06" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2lGADI7IA7dodvGMsIyPwA.png" data-width="1252" data-height="365" src="https://cdn-images-1.medium.com/max/800/1*2lGADI7IA7dodvGMsIyPwA.png"></figure><h3 name="99f5" id="99f5" class="graf graf--h3 graf-after--figure">Average Precision</h3><p name="01a8" id="01a8" class="graf graf--p graf-after--h3">The ideal value of precision and recall is 1. Since it is not possible to get perfect values, the closer the metrics ie precision and recall is to 1, the better our model is performing. Average precision is the average value of precision for the sampled points at various recall threshold values. The precision — recall curve for 3D object detection for the 3 classes i.e. cars, pedestrians and cyclists for all the three categories i.e. easy, moderate and hard is shown in Fig 3. The closer the curve is to the point (1,1), the higher performance of the model is.</p><figure name="6a5e" id="6a5e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*61jLdSnkQKkaaGqBDUW4Ag.png" data-width="951" data-height="355" src="https://cdn-images-1.medium.com/max/800/1*61jLdSnkQKkaaGqBDUW4Ag.png"></figure><p name="e6db" id="e6db" class="graf graf--p graf-after--figure">Finally we present the results for 3D object detection results on KITTI validation dataset in Fig 4.</p><figure name="e646" id="e646" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mgQ5D-XZIED464uWmZtVWg.png" data-width="1092" data-height="765" src="https://cdn-images-1.medium.com/max/800/1*mgQ5D-XZIED464uWmZtVWg.png"></figure><h3 name="60d1" id="60d1" class="graf graf--h3 graf-after--figure">Conclusions</h3><p name="024e" id="024e" class="graf graf--p graf-after--h3">In this blog, we presented our neural network for 3D object detection using LIDAR point cloud data. For making efficient computation, our architecture uses a single stage type neural network with bird’s view representation. We presented our architecture details, optimization, loss functions etc. We evaluated our network on the KITTI benchmark dataset. Our network outperforms previous state of the art approaches using average precision as the evaluation metric. The model runs at faster than 30 FPS thus making it a feasible option to be deployed in self driving cars.</p><h3 name="8ff1" id="8ff1" class="graf graf--h3 graf-after--p">References</h3><p name="7c4f" id="7c4f" class="graf graf--p graf-after--h3">X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun. Monocular 3d object detection for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2147–2156, 2016.</p><p name="4af3" id="4af3" class="graf graf--p graf-after--p">M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner. Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 1355–1361.</p><p name="061f" id="061f" class="graf graf--p graf-after--p">IEEE, 2017. A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3354–3361. IEEE, 2012.</p><p name="8dec" id="8dec" class="graf graf--p graf-after--p">R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014.</p><p name="10db" id="10db" class="graf graf--p graf-after--p">K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 37(9): 1904–1916, 2015.</p><h3 name="b99f" id="b99f" class="graf graf--h3 graf-after--p">Before You Go</h3><p name="9579" id="9579" class="graf graf--p graf-after--h3">Research Paper: <a href="https://arxiv.org/pdf/2006.01250.pdf" data-href="https://arxiv.org/pdf/2006.01250.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/2006.01250.pdf</a></p><p name="6b93" id="6b93" class="graf graf--p graf-after--p graf--trailing">Code: <a href="https://github.com/abhinavsagar/ldopc" data-href="https://github.com/abhinavsagar/ldopc" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/abhinavsagar/ldopc</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@abhinav.sagar" class="p-author h-card">Abhinav Sagar</a> on <a href="https://medium.com/p/ee0eb0e6389e"><time class="dt-published" datetime="2020-07-30T17:56:29.702Z">July 30, 2020</time></a>.</p><p><a href="https://medium.com/@abhinav.sagar/3d-object-detection-using-lidar-data-for-self-driving-cars-ee0eb0e6389e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 28, 2021.</p></footer></article></body></html>