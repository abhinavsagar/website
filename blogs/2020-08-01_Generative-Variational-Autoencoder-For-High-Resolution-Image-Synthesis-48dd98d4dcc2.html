<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Generative Variational Autoencoder For High Resolution Image Synthesis</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Generative Variational Autoencoder For High Resolution Image Synthesis</h1>
</header>
<section data-field="subtitle" class="p-summary">
Getting the best of both GANs and VAEs
</section>
<section data-field="body" class="e-content">
<section name="d69a" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8777" id="8777" class="graf graf--h3 graf--leading graf--title">Generative Variational Autoencoder For High Resolution Image Synthesis</h3><h4 name="e220" id="e220" class="graf graf--h4 graf-after--h3 graf--subtitle">Getting the best of both GANs and VAEs</h4></div><div class="section-inner sectionLayout--fullWidth"><figure name="0ed9" id="0ed9" class="graf graf--figure graf--layoutFillWidth graf-after--h4"><img class="graf-image" data-image-id="1*2INw4tc0SYN_fM-DRuIGQw.jpeg" data-width="6000" data-height="4000" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2560/1*2INw4tc0SYN_fM-DRuIGQw.jpeg"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@adaliphoto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/@adaliphoto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Manuel Velasquez</a> on <a href="https://unsplash.com/s/photos/high-resolution?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" data-href="https://unsplash.com/s/photos/high-resolution?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="d057" id="d057" class="graf graf--p graf-after--figure">This article presents our research on high resolution image generation using Generative Variational Autoencoder.</p><h3 name="684c" id="684c" class="graf graf--h3 graf-after--p">Important Points</h3><ol class="postList"><li name="d4cb" id="d4cb" class="graf graf--li graf-after--h3">Our work addresses the mode collapse issue of GANs and blurred images generated using VAEs in a single model architecture.</li><li name="c9a9" id="c9a9" class="graf graf--li graf-after--li">We use the encoder of VAE as it is while replacing the decoder with a discriminator.</li><li name="730e" id="730e" class="graf graf--li graf-after--li">The encoder is fed data from a normal distribution while the generator is fed that from a gaussian distribution.</li><li name="fe0b" id="fe0b" class="graf graf--li graf-after--li">The combination from both is then fed to a discriminator which tells whether the generated images are correct or not.</li><li name="9af3" id="9af3" class="graf graf--li graf-after--li">We evaluate our network on 3 different datasets: MNIST, CelebA-HQ and LSUN dataset.</li><li name="537d" id="537d" class="graf graf--li graf-after--li">We outperform previous state-of-the-art methods in terms of MMD, SSIM, log likelihood, reconstruction error, ELBO and KL divergence as the evaluation metrics.</li></ol><h3 name="4cca" id="4cca" class="graf graf--h3 graf-after--li">Introduction</h3><p name="a029" id="a029" class="graf graf--p graf-after--h3">The training of deep neural networks requires hundreds or even thousands of images. Lack of labelled datasets especially for medical images often hinders the progress. Hence it becomes imperative to create additional training data. Another area which is actively researched is using generative adversarial networks for image generation. Using this technique, new images can be generated by training on the existing images present in the dataset. The new images are realistic but different from the original data. There are two main approaches of using data augmentation using GANs: image to image translation and sampling from random distribution. The main challenge with GANs is the mode collapse problem i.e. the generated images are quite similar to each other and there is not enough variety in the images generated.</p><p name="929c" id="929c" class="graf graf--p graf-after--p">Another approach for image generation uses Variational Autoencoders. This architecture contains an encoder which is also known as generative network which takes a latent encoding as input and outputs the parameters for a conditional distribution of the observation. The decoder is also known as an inference network which takes as input an observation and outputs a set of parameters for the conditional distribution of the latent representation. During training VAEs use a concept known as reparameterization trick, in which sampling is done from a gaussian distribution. The main challenge with VAEs is that they are not able to generate sharp images.</p><h3 name="bfbd" id="bfbd" class="graf graf--h3 graf-after--p">Dataset</h3><p name="304c" id="304c" class="graf graf--p graf-after--h3">The following datasets are used for training and evaluation:</p><ol class="postList"><li name="119b" id="119b" class="graf graf--li graf-after--p">MNIST — This is a large dataset of handwritten digits which has been used successfully for training image classification and image processing algorithms. It contains 60,000 training images and 10,000 test images.</li><li name="1283" id="1283" class="graf graf--li graf-after--li">LSUN dataset — This dataset contains millions of color images with 10 scene categories and 20 object categories. This is one of the most common datasets for training and testing GAN based neural networks.</li><li name="d778" id="d778" class="graf graf--li graf-after--li">CelebA-HQ dataset -This is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. This is also one of the most common datasets for training and testing GAN based neural networks.</li></ol><h3 name="5e14" id="5e14" class="graf graf--h3 graf-after--li">VAE vs Ours Network</h3><p name="980d" id="980d" class="graf graf--p graf-after--h3">We show how instead of inference made in the way shown in original VAE architecture, we can add the error vector to the original data and multiply by standard distribution. The new term goes to the encoder and gets converted to the latent space. In the decoder, similarly the error vector gets added to the latent vector and multiplied by standard deviation. In this manner, we use the encoder of VAE in a manner similar to that in the original VAE. While we replace the decoder with a discriminator and hence change the loss function accordingly. The comparison between model architectures of VAE and our architecture is shown in Fig 1.</p><figure name="5db4" id="5db4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*bCAQClGTdq80kZl0hhDLKw.png" data-width="628" data-height="416" src="https://cdn-images-1.medium.com/max/800/1*bCAQClGTdq80kZl0hhDLKw.png"><figcaption class="imageCaption">Figure 1: Comparison between standard VAE and our network where e1 and e2 denote samples from some noise distribution, x denotes image vector, z denotes latent space vector, f and g denotes encoder and decoder functions respectively and +, ∗ denotes addition and concat operators.</figcaption></figure><p name="3b17" id="3b17" class="graf graf--p graf-after--figure">Our architecture can be seen both as an extension of VAE as well as that of GAN. Reasoning it as the former is easy as this requires a change in loss function for decoder, while the latter can be made by recalling the fact that GAN essentially works on the concept of zero sum game maintaining Nash Equilibrium between the generator and discriminator. In our case, both the encoder from VAE and discriminator from GAN are playing zero sum game and are competing with each other. As the training proceeds, the loss decreases in both the cases until it stabilizes.</p><h3 name="1a53" id="1a53" class="graf graf--h3 graf-after--p">Network Architecture</h3><p name="77d7" id="77d7" class="graf graf--p graf-after--h3">The network architecture used in this work is explained in the below points:</p><ol class="postList"><li name="6b90" id="6b90" class="graf graf--li graf-after--p">The discriminator and encoder networks have four convolution layers, each of which uses 3×3 filters.</li><li name="93a6" id="93a6" class="graf graf--li graf-after--li">We use Batch Normalization and Leaky Rectified Linear Unit (LeakyReLU) layers after each layer.</li><li name="2f27" id="2f27" class="graf graf--li graf-after--li">In training, we found that our architecture suffers from instability during training. This was solved using WGAN loss function which measures Wasserstein distance between two distributions.</li><li name="0341" id="0341" class="graf graf--li graf-after--li">We used the gradient penalty term to stabilize the training.</li><li name="6dc9" id="6dc9" class="graf graf--li graf-after--li">Our loss function has a total for 3 terms. While training, the encoder and the generator are considered as one network. Thus, we sum up the loss functions of the two networks in the order encoder-generator, discriminator as one and train the networks.</li><li name="f778" id="f778" class="graf graf--li graf-after--li">Two latent vectors are sampled one from normal distribution and the other from gaussian distribution. The one from normal distribution is fed to the encoder while the one from gaussian distribution is fed to the generator.</li><li name="264b" id="264b" class="graf graf--li graf-after--li">The outputs from both the vectors are in turn fed to the discriminator to tell whether the generated image is real or not.</li></ol><p name="9200" id="9200" class="graf graf--p graf-after--li">Our network architecture is shown in Fig 2.</p><figure name="da48" id="da48" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Nrvhq5jJxnokkKtf7ozvrw.png" data-width="652" data-height="216" src="https://cdn-images-1.medium.com/max/800/1*Nrvhq5jJxnokkKtf7ozvrw.png"><figcaption class="imageCaption">Figure 2: Our network architecture</figcaption></figure><h3 name="32c3" id="32c3" class="graf graf--h3 graf-after--figure">Architecture Details</h3><p name="86ee" id="86ee" class="graf graf--p graf-after--h3">The generator and discriminator layerwise architecture details is shown in Table 1 and Table 2 respectively. We denoted ResNet block as consisting of the following layers — convolutional, max pooling layer, 30 percent dropouts in between the layers and batch normalization layer.</p><figure name="a229" id="a229" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hmO1t2GzIk5HyWDScVdKig.png" data-width="792" data-height="428" src="https://cdn-images-1.medium.com/max/800/1*hmO1t2GzIk5HyWDScVdKig.png"></figure><figure name="1a4e" id="1a4e" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*Enw2r4a__457sing569f3g.png" data-width="750" data-height="359" src="https://cdn-images-1.medium.com/max/800/1*Enw2r4a__457sing569f3g.png"></figure><h3 name="2a77" id="2a77" class="graf graf--h3 graf-after--figure">Algorithm</h3><p name="b3eb" id="b3eb" class="graf graf--p graf-after--h3">The algorithm used in this work is trained using Stochastic Gradient Descent (SGD) as shown below:</p><figure name="22f7" id="22f7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mkfEOQNZJze3M2Cmr5Dv6w.png" data-width="1120" data-height="391" src="https://cdn-images-1.medium.com/max/800/1*mkfEOQNZJze3M2Cmr5Dv6w.png"></figure><h3 name="54dd" id="54dd" class="graf graf--h3 graf-after--figure">Experiments</h3><p name="6c05" id="6c05" class="graf graf--p graf-after--h3">All the generated samples are generator outputs from random latent vectors. We normalize all data into the range [-1, 1] and use two evaluation metrics to measure the performance of our network. First of them measures the distribution distance between the real and generated samples with maximum mean discrepancy (MMD) scores. The second metric evaluates the generation diversity with multi-scale structural similarity metric (MS-SSIM). Table 4. compares MMD and MS-SSIM scores with previous state of the art architectures.</p><figure name="3ce2" id="3ce2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Z0p3qFoQtMGGaFB5DOnSXg.png" data-width="821" data-height="251" src="https://cdn-images-1.medium.com/max/800/1*Z0p3qFoQtMGGaFB5DOnSXg.png"></figure><p name="60d2" id="60d2" class="graf graf--p graf-after--figure">We noticed the model with a small latent vector size of 100 suffers from severe mode collapse. The best results can be obtained using a moderately large latent vector size. Table 5 compares the effect of different latent variable sizes on the MMD and MS-SSIM scores respectively.</p><figure name="d297" id="d297" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*P0VFucdn6eiNd3y8PkoLSA.png" data-width="715" data-height="241" src="https://cdn-images-1.medium.com/max/800/1*P0VFucdn6eiNd3y8PkoLSA.png"></figure><p name="7f9a" id="7f9a" class="graf graf--p graf-after--figure">As can be seen, latent variable size with value 1000 produces the best results of those being compared. Both at low and high latent variable size mode collapse is seen which is one of the main challenges faced while training GANs.</p><p name="e3a2" id="e3a2" class="graf graf--p graf-after--p">Four common evaluation metrics have been used in the literature for testing the performance of generative models. These are log-likelihood, reconstruction error, ELBO and KL divergence.</p><p name="f54f" id="f54f" class="graf graf--p graf-after--p">The log-likelihood is calculated by finding the parameter that maximizes the log-likelihood of the observed sample. The reconstruction error is the distance between the original data point and its projection onto a lower-dimensional subspace. The optimization problem used in our model uses KL divergence error which is intractable hence we maximize ELBO instead of minimizing the KL divergence. KL divergence is a measure of how similar the generated probability distribution is to the true probability distribution. The comparison using these evaluation metrics for our model on MNIST dataset with the original VAE architecture is shown in Table 6.</p><figure name="1dc5" id="1dc5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WXV5MOXOH1aSE8nRlD4LOg.png" data-width="906" data-height="251" src="https://cdn-images-1.medium.com/max/800/1*WXV5MOXOH1aSE8nRlD4LOg.png"></figure><p name="f9fe" id="f9fe" class="graf graf--p graf-after--figure">We compare our log probability distribution value with those obtained by previous state of the art methods which is shown in Table 7. The log probability distribution is an important evaluation metric in the sense that it shows the diversity of the samples generated.</p><figure name="4f3f" id="4f3f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8FlI0EK84Tijez83grMJNg.png" data-width="829" data-height="340" src="https://cdn-images-1.medium.com/max/800/1*8FlI0EK84Tijez83grMJNg.png"></figure><h3 name="5f3b" id="5f3b" class="graf graf--h3 graf-after--figure">Results</h3><p name="4c5a" id="4c5a" class="graf graf--p graf-after--h3">We present the generated images on all the 3 datasets used for testing. The images were trained for 1000 iterations. The images generated using the CELEBA-HQ dataset is shown in Fig 3.</p><figure name="3d0f" id="3d0f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IXIC78h2maTLYFfEJDcghw.png" data-width="1044" data-height="342" src="https://cdn-images-1.medium.com/max/800/1*IXIC78h2maTLYFfEJDcghw.png"><figcaption class="imageCaption">Figure 3: 1024 × 1024 images generated using the CELEBA-HQ dataset.</figcaption></figure><p name="c666" id="c666" class="graf graf--p graf-after--figure">The images generated using the LSUN BEDROOM dataset is shown in Fig 4.</p><figure name="c256" id="c256" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ytc7M4ai0ebyNpXz6umuFg.png" data-width="1016" data-height="299" src="https://cdn-images-1.medium.com/max/800/1*ytc7M4ai0ebyNpXz6umuFg.png"><figcaption class="imageCaption">Figure 4: 256 × 256 images generated using LSUN BEDROOM dataset</figcaption></figure><p name="e618" id="e618" class="graf graf--p graf-after--figure">The images generated from different LSUN categories is shown in Fig 5.</p><figure name="790d" id="790d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-JIKyl4P2H5emZqzSCLgng.png" data-width="1065" data-height="330" src="https://cdn-images-1.medium.com/max/800/1*-JIKyl4P2H5emZqzSCLgng.png"><figcaption class="imageCaption">Figure 5: Sample 256 × 256 images generated from different LSUN categories</figcaption></figure><p name="6937" id="6937" class="graf graf--p graf-after--figure">We compare our results with previous state of the art networks on MNIST dataset in Fig 6.</p><figure name="998c" id="998c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*aGWqRVrhaDtHjl9Oi8iSbQ.png" data-width="408" data-height="332" src="https://cdn-images-1.medium.com/max/800/1*aGWqRVrhaDtHjl9Oi8iSbQ.png"><figcaption class="imageCaption">Figure 6: Generated MNIST images a) GAN b) WGAN c) VAE d) GVAE</figcaption></figure><h3 name="73e7" id="73e7" class="graf graf--h3 graf-after--figure">Conclusions</h3><p name="6418" id="6418" class="graf graf--p graf-after--h3">In this blog, we presented a new training procedure for Variational Autoencoders based on generative models. This allows us to make the inference model much more flexible, allowing it to represent almost any posterior distributions over the latent variables. Our network was trained and tested on 3 publicly available datasets. On evaluating using MMD, SSIM, log likelihood, reconstruction error, ELBO and KL divergence as the evaluation metrics, our network beats the previous state of the art algorithms. Using generative model approaches to generate additional training data especially in fields like medical imaging could be revolutionary as there is a shortage of medical data for training deep convolutional neural network architectures.</p><h3 name="8b2f" id="8b2f" class="graf graf--h3 graf-after--p">References</h3><p name="5250" id="5250" class="graf graf--p graf-after--h3">S. U. Dar, M. Yurt, L. Karacan, A. Erdem, E. Erdem, and T. Çukur. Image synthesis in multi-contrast mri with conditional generative adversarial networks. IEEE transactions on medical imaging, 38 (10):2375–2388, 2019.</p><p name="490c" id="490c" class="graf graf--p graf-after--p">I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014.</p><p name="1779" id="1779" class="graf graf--p graf-after--p">I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. In Advances in neural information processing systems, pages 5767–5777, 2017.</p><p name="9aa4" id="9aa4" class="graf graf--p graf-after--p">D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p><h3 name="19ac" id="19ac" class="graf graf--h3 graf-after--p">Before You Go</h3><p name="4363" id="4363" class="graf graf--p graf-after--h3">Research Paper: <a href="https://arxiv.org/pdf/2008.10399.pdf" data-href="https://arxiv.org/pdf/2008.10399.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/2008.10399.pdf</a></p><p name="e19e" id="e19e" class="graf graf--p graf-after--p graf--trailing">Code: <a href="https://github.com/abhinavsagar/gvae" data-href="https://github.com/abhinavsagar/gvae" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/abhinavsagar/gvae</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@abhinav.sagar" class="p-author h-card">Abhinav Sagar</a> on <a href="https://medium.com/p/48dd98d4dcc2"><time class="dt-published" datetime="2020-08-01T18:44:45.667Z">August 1, 2020</time></a>.</p><p><a href="https://medium.com/@abhinav.sagar/generative-variational-autoencoder-for-high-resolution-image-synthesis-48dd98d4dcc2" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on April 28, 2021.</p></footer></article></body></html>